<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>Ceph 基础及Ceph Pacific集群部署 | TheOpsCafe</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://vincentgresham.github.io/favicon.ico?v=1641177955391">
<link rel="stylesheet" href="https://vincentgresham.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="本节重点

ceph的组件和功能
ceph的数据读写流程
使用ceph-deploy部署ceph集群
测试ceph的rbd使用

服务器初始化
分布式存储概述

单机存储

即将磁盘直接安装到物理服务器中，可能存在的问题：磁盘IO问题、扩容..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://vincentgresham.github.io">
        <img src="https://vincentgresham.github.io/images/avatar.png?v=1641177955391" class="site-logo">
        <h1 class="site-title">TheOpsCafe</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://vincentgresham.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">Ceph 基础及Ceph Pacific集群部署</h2>
            <div class="post-date">2021-09-01</div>
            
            <div class="post-content" v-pre>
              <h1 id="本节重点">本节重点</h1>
<ul>
<li>ceph的组件和功能</li>
<li>ceph的数据读写流程</li>
<li>使用ceph-deploy部署ceph集群</li>
<li>测试ceph的rbd使用</li>
</ul>
<h1 id="服务器初始化">服务器初始化</h1>
<h1 id="分布式存储概述">分布式存储概述</h1>
<ul>
<li>单机存储</li>
</ul>
<p>即将磁盘直接安装到物理服务器中，可能存在的问题：磁盘IO问题、扩容问题、高可用问题</p>
<ul>
<li>商业存储</li>
</ul>
<p>EMC、NetAPP、戴尔、华为、浪潮等厂商的商业化存储。</p>
<ul>
<li>分布式存储</li>
</ul>
<p>软件定义存储（Software Defined Storage, SDS），使用动态、敏捷和自动化软件定义的存储代替低效（<s>昂贵</s>）的专用存储硬件，以提高效率并降低成本。</p>
<p>常用的非商业分布式存储系统有：Ceph、TFS（Taobao FileSystem）、FastDFS、MogileFS（Memcached公司Danga开发）、MooseFS，GlusterFS（被RedHat收购），关于上述分布式存储系统的介绍可参考 :</p>
<p><a href="https://segmentfault.com/a/1190000039196722">盘点分布式文件存储系统</a></p>
<figure data-type="image" tabindex="1"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9760e40b-2f21-4376-b7af-557ed26dbd6a/%E6%88%AA%E5%B1%8F2021-08-15_%E4%B8%8B%E5%8D%885.47.12.png" alt="截屏2021-08-15 下午5.47.12.png" loading="lazy"></figure>
<h3 id="有状态集群数据读写特性">有状态集群数据读写特性</h3>
<p>数据分为读数据和写数据，读可以在任意一个节点读，但写只能在特定的节点写。如Redis的master、zookeeper的leader、MySQL的master等场景</p>
<h2 id="分布式存储的数据特性">分布式存储的数据特性</h2>
<p>数据保存时，可分为&quot;数据&quot;和&quot;元数据&quot;两部分，元数据是文件的属性信息，以HDFS为例，Name Node提供文件元数据的路由功能，告诉应用去哪个服务器去请求文件内容。Data Node实际存储诗句、执行数据的读写请求以及数据的高可用功能。</p>
<figure data-type="image" tabindex="2"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2558338f-9d60-4328-b8df-eb1fe79eb23c/Untitled.png" alt="Untitled" loading="lazy"></figure>
<p>在部署分布式存储时，一般会将数据同步端口和业务端口分离，防止网络带宽/稳定性瓶颈。存储数据时是一主两备份，以实现数据的高可用。副本池中包含3个副本，也可以5个副本，副本越多越占用空间，但是数据安全性也更高，副本数量多的情况下会有写放大的问题（副本越多，完成同步写所需的时长越长）。</p>
<h2 id="存储类型">存储类型</h2>
<p>根据存储的类型，可以分为块存储、文件存储、对象存储：</p>
<ul>
<li>块存储：需要格式化，将文件直接保存在磁盘上。</li>
<li>文件存储：提供数据存储的接口，由操作系统对块存储控制，由操作系统提供存储接口，应用程序通过调用操作系统将文件保存到块存储进行持久化。</li>
<li>对象存储：文件被拆分为若干个部分分散在多个存储服务器。在对象存储中，数据被拆解为成为”对象“的离散单元，并保存在单个存储库中，而不是作为文件夹中的文件或服务器上的块来保存。需要一个接口用于上传下载。</li>
</ul>
<h1 id="ceph-简介">Ceph 简介</h1>
<p>Ceph是一个开源的分布式存储系统，同时支持对象存储、块存储、文件系统存储，所以也称之为 统一存储。</p>
<p>Ceph是支持提供对象式（Object）存储，它把每一个待管理的数据流（文件等数据）切分为一到多个固定大小（默认4M）的对象数据，并以其为原子单元（原子是构成元素的最小单元）完成数据的读写。</p>
<p>对象数据的底层存储是由多个存储主机（host）组成的存储集群，该集群也被称之为RADOS(reliable automatic distributed object store)存储集群，即可靠的、自动化的、分布式的对象存储。librados是RADOS存储集群的API，支持C/C++/JAVA/Python/Ruby/PHP 等编程语言客户端。</p>
<p>Ceph使用RADOS提供对象存储，通过librados封装库提供多种存储方式的文件和对象转换。外层通过RGW（Object对象存储接口，有原生API，也兼容Swift和S3的API）提供对象存储服务、RBD（Block块存储，支持精简配置、快照、克隆，适合多客户端有目录结构）提供块存储服务、CephFS（File文件存储，Posix接口，支持快照，适合变动少的数据，没有目录结构不能直接打开）提供文件存储服务。</p>
<figure data-type="image" tabindex="3"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a443a80b-f636-49b0-ace8-53c7c5fe5066/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h2 id="ceph-设计思想">ceph 设计思想</h2>
<ul>
<li>每一组件皆可扩展</li>
<li>无单点故障</li>
<li>基于软件而且开源，没有厂商锁定</li>
<li>在现有廉价硬件上运行</li>
<li>尽可能自动管理，减少用户干预</li>
</ul>
<h2 id="ceph-版本">ceph 版本</h2>
<p>Ceph是圣克鲁兹加利福尼亚大学的Sage Weil在2003年开发的，也是他的博士学位项目的一部分。初始的项目原型是大约40000行C++代码的Ceph文件系统，并于2006年作为参考实现和研究平台遵循LGPL协议（Lesser GUN Public License）开源。美国劳伦斯利物莫国家实验室（Lawrence Livermore National Laboratory）资助了Sage的初始研究工作。2003～2007年是Ceph的研究开发时期。在这期间，它的核心组件逐步形成，并且社区对项目的贡献也已经开始逐渐变大。Ceph没有采用双重许可模式，也就不存在只针对企业版的特性。</p>
<p>Ceph的第一个版本是0.1，发布日期是2008.1，其版本命名遵循以下规则：</p>
<ul>
<li>x.0.z - 开发版 用于开发者、早期测试者和勇士</li>
<li>x.1.z - 候选版 用于测试集群、高手</li>
<li>x.2.z - 稳定，修正版 用户发布给用户</li>
</ul>
<p>x将从9算起，它代表Infernalis（首字母I是英文单词中的第9个字母），则对应的开发版本：</p>
<ul>
<li>9.0.0,9.0.1,9.0.2 开发版</li>
<li>9.1.0,9.1.1,9.1.2 候选版</li>
<li>9.2.0,9.2.1,9.2.2 稳定版</li>
</ul>
<p><a href="https://www.notion.so/98fbc4b3058549cbb0ea81b2d2e10529">Active Ceph Releases</a></p>
<h2 id="ceph-核心术语">Ceph 核心术语</h2>
<ul>
<li>RADOS：Reliable Autonomic Distributed Object Store, 可靠的、自动化的、分布式对象存储系统。RADOS是Ceph集群的核心，用于实现数据分配、故障转移恢复等集群操作</li>
<li>librados：将RADOS封装为一个库，方便上层的RBD、RGW和CephFS调用，支持PHP、Ruby、Java、Python、C和C++。</li>
<li>CRUSH：Controlled Replication Under Scalable Hashing，一种算法，Ceph使用CRUSH算法计算对象存储位置，摒弃了传统的集中式存储元数据寻址方案。Ceph将数据通过对象的形式存储在逻辑存储池（Pool）中，通过CRUSH算法计算出哪一个PG应该存放对象，并进一步计算出哪些OSD应该存放PG。</li>
<li>Pool：存储对象的逻辑分区，规定了数据冗余的类型和对应的副本分布策略，支持两种类型：副本(replicated)和纠删码(Erasure Code)。一个pool内部可以有多个PG。</li>
<li>PG：Placement Group，放置策略组，一系列对象的集合，在该集合内的所有对象都具有相同的放置策略。PG是Ceph中的逻辑概念，服务端数据均衡和恢复的最小粒度就是PG，一个PG包含多个OSD。引入PG是为了更好地分配和定位数据。</li>
<li>Object：最底层的存储单元，包含元数据和初始数据。</li>
</ul>
<p>其余术语，可参考 ：</p>
<p><a href="https://docs.ceph.com/en/latest/glossary/"></a></p>
<h2 id="ceph-组件">Ceph 组件</h2>
<figure data-type="image" tabindex="4"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c904316c-d910-4fba-880b-cf29f15679b8/Untitled.png" alt="Untitled" loading="lazy"></figure>
<p>Ceph 主要包含四个组件：Monitors、Managers、Ceph OSDs、MDSs</p>
<h3 id="ceph-monitorsceph-mon-监视器">Ceph Monitors(ceph-mon) 监视器</h3>
<p>Ceph集群的监视器，一个Ceph集群需要多个Monitors组成的小集群，通过Paxos同步数据，用于保存OSD的元数据。负责监控整个Ceph集群运行时的Map视图（OSD Map、Monitor Map、PG Map、CRUSH Map）。同时还管理集群客户端认证和授权。通常一个Ceph集群中至少需要3个Mon节点来保证冗余和高可用。</p>
<h3 id="managersceph-mgr-管理器">Managers(ceph-mgr) 管理器</h3>
<p>Ceph Manager 守护进程负责跟踪运行时指标和Ceph集群的当前状态，包括当前的存储利用率、性能指标、系统负载。同时也提供基于Python编写的Ceph Dashboard Web和REST API，可供外部调用如：cephmetrics,zabbix,prometheus等。通常至少需要2个Mgr节点来保证集群冗余和高可用性。</p>
<h3 id="ceph-osdsceph-osd">Ceph OSDs(ceph-osd)</h3>
<p>Ceph Object Storage Daemons 对象存储守护进程,负责物理存储的进程，一般配置为与磁盘一一对应，一块磁盘启动一个OSD进程。 用于存储数据，处理集群数据复制、恢复、重新平衡，并检查其他OSD守护程序的心跳来向Ceph Mon和MGR提供监控信息。通常至少需要3个OSD节点来保证冗余和高可用性。</p>
<ul>
<li>一个Pool里有很多PG</li>
<li>一个PG里包含一堆对象，一个对象只能属于一个PG</li>
<li>PG有主从之分，一个PG分布在不同的OSD上（三副本）</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6a91c06d-cbcd-4e2a-9ffb-4ff7bd102ea5/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h3 id="ceph-mdssceph-mds">Ceph MDSs(ceph-mds)</h3>
<p>Ceph Metadata Server 元数据服务器，用于Ceph文件存储（Ceph-FS），Ceph块存储和对象存储不使用MDS。负责保存文件系统的元数据，管理目录结构。</p>
<p>因此，一个Ceph集群，应至少包含：</p>
<ul>
<li>至少需要一个Ceph Monitors 监视器（1，3，5，7...奇数）</li>
<li>两个或以上的Ceph管理器Managers</li>
<li>3个以上Ceph OSD（对象存储守护程序）</li>
<li>若需要运行Ceph文件系统客户端时，还需要高可用的Ceph Metadata Server（文件系统元数据服务器）</li>
</ul>
<h2 id="ceph-数据存储过程">Ceph 数据存储过程</h2>
<p>Ceph集群部署好之后，要先创建存储池才能向ceph写入数据，文件在向ceph保存之前要先进行一致性hash计算，计算后会把文件保存在某个对应的PG，此文件一定属于某个pool的一个PG，再通过PG保存在OSD上。数据对象在写到主OSD之后再同步到从OSD以实现数据的高可用。</p>
<figure data-type="image" tabindex="6"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bc5eeb52-9f11-4946-b69e-99e183e8bbd9/%E6%88%AA%E5%B1%8F2021-08-15_%E4%B8%8B%E5%8D%887.22.49.png" alt="截屏2021-08-15 下午7.22.49.png" loading="lazy"></figure>
<p>以RBD块存储为例，完整数据写入Ceph完整流程是：</p>
<p><strong>File→ (Pool, Object) → (Pool PG) → OSD set → OSD Disk</strong></p>
<ol>
<li>客户端从mon获取最新的cluster map</li>
<li>客户端通过librdb对二进制进行分块，映射为RADOS能够处理的Object（File→Object）
<ul>
<li>oid(object id) = ino(inode number) + ono(object number)</li>
<li>ino为file的元数据序列号，是file的唯一id。oid是由file产生的某个object的序号，默认以4M大小切分。</li>
<li>按照Object的最大Size进行切分，有两个好处：
<ul>
<li>让大小不一的file变成最大size一致，可以被RADOS高效管理的Object。</li>
<li>对单一file的串行处理变为对多个Object的并行处理。</li>
</ul>
</li>
</ul>
</li>
<li>客户端基于hash算法计算出object将要存储的PG的ID（Object→PG）
<ul>
<li>file被映射为一个或多个object之后，就需要将每个object独立地映射到一个PG中。需要通过hash算法计算出PG的id号，计算公式为：PG_ID = hash(pool_id).hash(object_id) % PG 总数
<ol>
<li>客户端输入pool ID和object ID（比如pool=&quot;mypool&quot;, object_id = &quot;myfile&quot;)</li>
<li>ceph对object ID 做哈希</li>
<li>ceph对该hash值取PG总数的模，得到PG编号，比如50（第2、3步基本保证一个pool中的所有PG会被均匀地使用）</li>
<li>ceph对pool ID取hash（比如 &quot;mypool&quot; = 1）</li>
<li>ceph 将pool ID和PG ID组合在一起（比如1.50）得到PG的完整ID。</li>
</ol>
</li>
<li>根据RADOS的设计，上述计算整体结果是从所有m个PG中近似均匀地随机选择一个。基于这个机制，当有大量object和大量PG时，RADOS能保证object和PG之间的近似均匀映射。</li>
</ul>
</li>
<li>客户端根据CRUSH算法计算出PG中目标的主/次 OSD的ID （PG → OSD）
<ul>
<li>将object的逻辑组织单元的PG映射到数据的实际存储单元OSD</li>
<li>在创建pool时，PG与OSD的对应关系就确定了，这里只是CRUSH查找。</li>
<li>计算公式：OSD_IDS=CRUSH(PG_ID, cluster_map, crush_rules)</li>
<li>计算得到的OSD_IDS包含n个OSD，这些OSDs共同负责存储和维护一个PG中的所有Object。n的数值可以根据实际应用中对于可靠性的需求而配置，生产环境下通常为3。</li>
<li>具体到每个OSD，则由对应OSD Daemon负责执行映射到本地的object在本地文件系统中进行存储、访问、元数据维护等操作。</li>
</ul>
</li>
</ol>
<p>将上述完整流程概述为如下：</p>
<ol>
<li>客户端需要向ceph中写入一个file，首先在本地完成寻址流程，将file变成n个object，然后找出存储每个object的对应一组三个OSD。</li>
<li>找出三个OSD后，客户端将直接和Primary OSD通信，发起写入请求。Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写入操作。Secondary/Tertiary OSD各自完成写入操作后，发送ACK确认信息给Primary OSD。当Primary OSD确认其他两个OSD写入完成后，则认为当前数据完全写入，向客户端发送确认信息，object写入完成。</li>
</ol>
<figure data-type="image" tabindex="7"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8297bb3f-bc53-4ba7-abf4-e228c13d4ee8/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h2 id="ceph-元数据保存方式">Ceph 元数据保存方式</h2>
<p>Ceph对象数据的元数据以KV的形式存在，在RADOS中有以下两种实现：</p>
<ul>
<li>
<p>xattrs 扩展属性：将元数据保存在文件的扩展属性中并保存在系统磁盘上，要求支持对象存储的本地文件系统支持扩展属性（一般为XFS）</p>
</li>
<li>
<p>omap 对象映射（KV）：Object Map 对象映射表，将元数据保存在本地文件系统之外独立的KV存储系统中。</p>
<ul>
<li>filestore与leveldb（google开源）</li>
<li>bluestore与rocksdb （facebook开源）：当前常用</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7083d39d-8294-454c-8502-e8f62f0f0140/Untitled.png" alt="Untitled" loading="lazy"></figure>
</li>
</ul>
<p>ceph后端可以选择多种存储引擎，比如filestore、bluestore等存储对象数据的元数据信息。</p>
<p>参考文档：</p>
<p><a href="https://zhuanlan.zhihu.com/p/41909971">Ceph读写性能估算方法</a></p>
<h2 id="ceph-crush-算法">Ceph CRUSH 算法</h2>
<p>Controller replication under scalable hashing，一种分布式算法，实时计算，在mon上运算。</p>
<p>若将对象直接映射到OSD上会导致对象与OSD之间的对应关系过于紧密和偶尔，OSD故障时会对整个ceph集群产生影响。因此ceph将一个对象映射到RADOS集群时：</p>
<ul>
<li>首先使用一致性哈希算法将对象名称映射到PG</li>
<li>将PG ID基于CRUSH算法映射到OSD</li>
</ul>
<p>上述两步操作都是以“实时计算”的方式完成，有效避免了组件“中心化”的问题，也解决了查询性能和冗余问题，使ceph集群扩展不再受查询的性能限制。</p>
<h1 id="部署ceph-集群">部署Ceph 集群</h1>
<p>在企业生产环境中部署，单台对应角色主机至少需要以下配置：</p>
<ul>
<li>ceph-mon：16C 16G 200G</li>
<li>ceph-mgr：32C 32G 200G （启用块存储和对象存储）</li>
<li>ceph-osd：使用企业级ssd，多块PCIE万兆网卡</li>
</ul>
<h2 id="服务器初始化-2">服务器初始化</h2>
<p>需要先完成以下操作：</p>
<ul>
<li>关闭selinux和防火墙</li>
<li>配置好时间同步</li>
</ul>
<pre><code class="language-bash">*/5 * * * *  /usr/sbin/ntpdate cn.pool.ntp.org &gt;/dev/null 2&gt;&amp;1
</code></pre>
<ul>
<li>配置域名解析或DNS解析，将所有节点的hostname、fqdn写入hosts文件</li>
<li>配置免密登录</li>
</ul>
<pre><code class="language-bash">### 使用Ubuntu 18.04 版本部署
### 修改安装时的内核参数（F6），使网卡名称修改为eth0
net.ifnames=0 biosdevname=0

### 已安装操作系统修改修改网卡名称为eth*
ops@ubuntu1804:~# sudo vim /etc/default/grub
GRUB_DEFAULT=0
GRUB_TIMEOUT_STYLE=hidden
GRUB_TIMEOUT=2
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;net.ifnames=0 biosdevname=0&quot;
GRUB_CMDLINE_LINUX=&quot;&quot;
ops@ubuntu1804:~# sudo update-grub

### 如果已修改过网卡配置，则需要更新配置文件中的网卡名
ops@ubuntu1804:~# vim /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: no
      addresses: [10.0.11.102/16]
      optional: true
      gateway4: 10.0.1.1
      nameservers:
        addresses: [114.114.114.114,8.8.8.8]
    eth1:
      dhcp4: no
      addresses: [192.168.0.102/24]
      optional: true
      nameservers:
        addresses: [114.114.114.114,8.8.8.8]

### 配置允许root登录，并为root用户设置密码
# vim /etc/ssh/sshd_config
PermitRootLogin yes
# passwd
# systemctl restart sshd.service

### 安装常用软件包
# apt install iproute2 ntpdate tcpdump telnet traceroute nfs-kernel-server nfs-common lrzsz tree openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev ntpdate tcpdump telnet traceroute gcc openssh-server lrzsz tree openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev ntpdate tcpdump telnet traceroute iotop unzip zip -y

### 部署时间同步服务
# apt install -y chrony
### 修改 ceph-deploy 节点chrony配置，使其作为ntp server
# vim /etc/chrony/chrony.conf
allow 192.168.0.0/24
### 修改所有其他节点配置，使ntp server指向ceph-deploy节点
# vim /etc/chrony/chrony.conf
	pool 192.168.0.100        iburst maxsources 4
	#pool ntp.ubuntu.com        iburst maxsources 4
	#pool 0.ubuntu.pool.ntp.org iburst maxsources 1
	#pool 1.ubuntu.pool.ntp.org iburst maxsources 1
	#pool 2.ubuntu.pool.ntp.org iburst maxsources 2

# systemctl restart chrony
# timedatectl set-timezone Asia/Shanghai
# chronyc -a makestep
### 验证时间同步配置
# chronyc sources -v

ops@ubuntu1804:~# sudo reboot

</code></pre>
<p>在如下测试环境中，使用Ubuntu 18.04 LTS部署，包含1个ceph-deploy节点用于部署ceph，2个ceph-mgr节点，3个ceph-mon节点，4个ceph-osd节点。</p>
<p><a href="https://www.notion.so/d4d2b4fc450845f0ac0a3933a0a85577">Ceph Cluster IP信息</a></p>
<h3 id="ceph-集群部署ceph-deploy">Ceph 集群部署(ceph-deploy)</h3>
<ol>
<li>按照服务器初始化步骤，配置好服务器基础环境和IP。</li>
<li>在所有ceph节点中创建普通用户ceph-admin，用于管理ceph（也可以使用系统本身的非特权账号）。在部署ceph时，会自动创建名为ceph账号，ceph-admin账号用于部署和管理ceph。</li>
</ol>
<pre><code class="language-bash">all-node# groupadd -r -g 2023 ceph-admin &amp;&amp; useradd -r -m -s /bin/bash -u 2023 -g 2023 ceph-admin &amp;&amp; echo ceph-admin:1s1sljj | chpasswd
${一个密码}

all-node# usermod -aG sudo ceph-admin

### 配置ceph-admin账户免密登录其余ceph节点
ceph-deploy# su - ceph-admin
ceph-deploy# ssh-keygen -t rsa
ceph-deploy# ssh-copy-id ceph-admin@ceph-deploy
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon1
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon2
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon3
ceph-deploy# ssh-copy-id ceph-admin@ceph-mgr1
ceph-deploy# ssh-copy-id ceph-admin@ceph-mgr2
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd1
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd2
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd3
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd4

### 在所有ceph节点中部署python2 
ssh ceph-admin@ceph-mon1 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;
ssh ceph-admin@ceph-mon2 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;
...
ssh ceph-admin@ceph-osd4 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;

### 在所有节点中部署 ceph-common 16.2.5-1bionic组件
ssh ceph-admin@ceph-mon1 &quot;sudo apt install ceph-common -y&quot;
ssh ceph-admin@ceph-mon2 &quot;sudo apt install ceph-common -y&quot;
...
ssh ceph-admin@ceph-osd4 &quot;sudo apt install ceph-common -y&quot;

### 在添加osd之前，部署osd节点的基本环境，添加P版的源配置（来源：https://mirrors.tuna.tsinghua.edu.cn/help/ceph）* （重要，否则可能由于apt源中的ceph组件版本不同而导致集群创建失败）

ceph-admin@ceph-deploy:/etc/ceph$ for i in ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
do
ssh ceph-admin@$i &quot;wget -q -O- 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc' | sudo apt-key add -&quot;
ssh ceph-admin@$i &quot;echo 'deb https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic main' | sudo tee -a /etc/apt/sources.list&quot;
ssh ceph-admin@$i &quot;sudo apt-get update&quot;
done
</code></pre>
<ol>
<li>使用ceph-deploy部署ceph</li>
</ol>
<pre><code class="language-bash">ceph-deploy:~# apt-cache madison ceph-deploy
ceph-deploy |      2.0.1 | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages
ceph-deploy |      2.0.1 | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main i386 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe amd64 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe i386 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe Sources
ceph-deploy:~# sudo apt install ceph-deploy
ceph-deploy:~# sudo apt install ceph-common # 用于管理ceph，执行ceph -s
</code></pre>
<ol>
<li>初始化部署mon</li>
</ol>
<pre><code class="language-bash">ops@ceph-deploy:~$ ceph-deploy --help

### 各个ceph节点上部署python2.7
mon:~# apt install python2.7 -y
mon:~# ln -sv /usr/bin/python2.7 /usr/bin/python2

### 部署新mon节点
ceph-admin@ceph-deploy:~$ mkdir ceph-cluster &amp;&amp; cd $_

ceph-admin@ceph-deploy:~$ ceph-deploy new --cluster-network 192.168.0.0/24 --public-network 10.0.11.0/16 ceph-mon1 ceph-mon2 ceph-mon3

ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 20
drwxrwxr-x 2 ceph-admin ceph-admin 4096 Aug 16 10:47 ./
drwxr-xr-x 6 ceph-admin ceph-admin 4096 Aug 16 10:47 ../
-rw-rw-r-- 1 ceph-admin ceph-admin  261 Aug 16 10:47 ceph.conf  ## 自动生成的配置文件
-rw-rw-r-- 1 ceph-admin ceph-admin 3327 Aug 16 10:47 ceph-deploy-ceph.log  ## 初始化日志
-rw------- 1 ceph-admin ceph-admin   73 Aug 16 10:47 ceph.mon.keyring  ## 用于ceph mon节点内部通讯认证的秘钥环文件

ceph-admin@ceph-deploy:~/ceph-cluster$ cat ceph.conf
[global]
fsid = cb440925-c0f5-49a1-a9f2-ae9470ccd17e
public_network = 10.0.11.0/16
cluster_network = 192.168.0.0/24
mon_initial_members = ceph-mon1, ceph-mon2, ceph-mon3
mon_host = 10.0.11.101,10.0.11.102,10.0.11.103
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

### 在mon节点安装组件ceph-mon 16.2.5-1bionic，并初始化mon节点
ceph-mon1 # apt install ceph-mon=16.2.5-1bionic -y
ceph-mon2 # apt install ceph-mon=16.2.5-1bionic -y
ceph-mon3 # apt install ceph-mon=16.2.5-1bionic -y

### 初始化mon节点,将会读取ceph.conf中mon节点配置，初始化mon
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy mon create-initial

### 检查mon daemon是否已运行
root@ceph-mon1:~# ps -ef|grep ceph-mon
ceph      7192     1  0 10:51 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-mon1 --setuser ceph --setgroup ceph
root      7892  3440  0 10:51 pts/0    00:00:00 grep --color=auto ceph-mon

##  推送ceph配置和keyring到ceph中其他节点,即可以使用其他节点管理ceph
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy admin ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4

### 检查ceph集群状态，报错是由于ceph-admin用户没有权限读 /etc/ceph/ 路径下的文件
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
2021-08-16T10:52:47.568-0400 7fc6f204c700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory

### 修复该问题，在所有节点上添加ceph-admin账号的读写权限
ceph-admin@ceph-deploy:/etc/ceph$ for i in ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
do
ssh ceph-admin@$i &quot;sudo setfacl -m u:ceph-admin:rw /etc/ceph/ceph.client.admin.keyring&quot;
done

### 检查ceph集群状态
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_WARN
            mons are allowing insecure global_id reclaim

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 40s)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

### 解决 WARN 报错,禁用非安全模式通信
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph config set mon auth_allow_insecure_global_id_reclaim false

ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 2m)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

</code></pre>
<ol>
<li>初始化部署mgr</li>
</ol>
<pre><code class="language-bash">### 部署mgr服务
ceph-admin@ceph-mgr1:~$ sudo apt install ceph-mgr=16.2.5-1bionic -y
ceph-admin@ceph-mgr2:~$ sudo apt install ceph-mgr=16.2.5-1bionic -y

ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy mgr create ceph-mgr1 ceph-mgr2

### 验证mgr服务
ceph-admin@ceph-mgr1:~$ ps -ef | grep mgr
ceph      8745     1  0 11:15 ?        00:00:00 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-mgr1 --setuser ceph --setgroup ceph
ceph-ad+  8815  6069  0 11:16 pts/0    00:00:00 grep --color=auto mgr
</code></pre>
<ol>
<li>初始化部署osd</li>
</ol>
<pre><code class="language-bash">~~### 部署ceph-osd节点，将按照串行依次安装ceph-common、ceph-radosgw 等基本组件
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy install --no-adjust-repos --nogpgcheck ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
[ceph-osd4][INFO  ] Running command: sudo ceph --version
[ceph-osd4][DEBUG ] ceph version 16.2.5 (0883bdea7337b95e4b611c768c0279868462204a) pacific (stable)~~

### 初始化osd节点, 将会安装ceph-common, ceph-radosgw, ceph-osd等组件，版本需均为 16.2.5-1bionic
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy install --release pacific ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4

# 列出node节点上的磁盘
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy disk list ceph-osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy disk list ceph-osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  debug                         : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa41b24afa0&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph-osd1']
[ceph_deploy.cli][INFO  ]  func                          : &lt;function disk at 0x7fa41b2262d0&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph-osd1][DEBUG ] connection detected need for sudo
[ceph-osd1][DEBUG ] connected to host: ceph-osd1
[ceph-osd1][DEBUG ] detect platform information from remote host
[ceph-osd1][DEBUG ] detect machine type
[ceph-osd1][DEBUG ] find the location of an executable
[ceph-osd1][INFO  ] Running command: sudo fdisk -l
[ceph-osd1][INFO  ] Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors
[ceph-osd1][INFO  ] Disk /dev/sdb: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdc: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdd: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdf: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sde: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/mapper/ubu1804--vg-root: 99 GiB, 106342383616 bytes, 207699968 sectors
[ceph-osd1][INFO  ] Disk /dev/mapper/ubu1804--vg-swap_1: 980 MiB, 1027604480 bytes, 2007040 sectors

### 擦除osd磁盘(根据osd节点实际磁盘进行擦除)
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy disk zap $ceph-node $/dev/sdb
### ceph_osdzap.sh
#!/bin/bash
ceph-deploy disk zap ceph-osd1 /dev/sdb
ceph-deploy disk zap ceph-osd1 /dev/sdc
ceph-deploy disk zap ceph-osd1 /dev/sdd
ceph-deploy disk zap ceph-osd1 /dev/sde
ceph-deploy disk zap ceph-osd1 /dev/sdf

ceph-deploy disk zap ceph-osd2 /dev/sdb
ceph-deploy disk zap ceph-osd2 /dev/sdc
ceph-deploy disk zap ceph-osd2 /dev/sdd
ceph-deploy disk zap ceph-osd2 /dev/sde
ceph-deploy disk zap ceph-osd2 /dev/sdf

ceph-deploy disk zap ceph-osd3 /dev/sdb
ceph-deploy disk zap ceph-osd3 /dev/sdc
ceph-deploy disk zap ceph-osd3 /dev/sdd
ceph-deploy disk zap ceph-osd3 /dev/sde
ceph-deploy disk zap ceph-osd3 /dev/sdf

ceph-deploy disk zap ceph-osd4 /dev/sdb
ceph-deploy disk zap ceph-osd4 /dev/sdc
ceph-deploy disk zap ceph-osd4 /dev/sdd

### 添加osd, osd id 将从0开始，一块磁盘对应一个osd
ceph-admin@ceph-deploy:~/ceph-cluster$  ceph-deploy osd create $ceph-node --data $/dev/sdb...
### ceph_osdcreate.sh
#!/bin/bash
ceph-deploy osd create ceph-osd1 --data /dev/sdb
ceph-deploy osd create ceph-osd1 --data /dev/sdc
ceph-deploy osd create ceph-osd1 --data /dev/sdd
ceph-deploy osd create ceph-osd1 --data /dev/sde
ceph-deploy osd create ceph-osd1 --data /dev/sdf

ceph-deploy osd create ceph-osd2 --data /dev/sdb
ceph-deploy osd create ceph-osd2 --data /dev/sdc
ceph-deploy osd create ceph-osd2 --data /dev/sdd
ceph-deploy osd create ceph-osd2 --data /dev/sde
ceph-deploy osd create ceph-osd2 --data /dev/sdf

ceph-deploy osd create ceph-osd3 --data /dev/sdb
ceph-deploy osd create ceph-osd3 --data /dev/sdc
ceph-deploy osd create ceph-osd3 --data /dev/sdd
ceph-deploy osd create ceph-osd3 --data /dev/sde
ceph-deploy osd create ceph-osd3 --data /dev/sdf

ceph-deploy osd create ceph-osd4 --data /dev/sdb
ceph-deploy osd create ceph-osd4 --data /dev/sdc
ceph-deploy osd create ceph-osd4 --data /dev/sdd

### 验证osd节点服务
root@ceph-osd1:~# ps -ef| grep osd
ceph       20905       1  0 11:34 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph
ceph       22649       1  0 11:34 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph
ceph       24414       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph
ceph       26150       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph
ceph       27883       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph
root       28410    6825  0 11:35 pts/0    00:00:00 grep --color=auto osd

### 验证ceph集群状态
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 14m)
    mgr: ceph-mgr1(active, since 9m), standbys: ceph-mgr2
    osd: 18 osds: 18 up (since 10s), 18 in (since 19s)

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   116 MiB used, 288 GiB / 288 GiB avail
    pgs:     1 active+clean
</code></pre>
<h2 id="测试上传于下载数据">测试上传于下载数据</h2>
<pre><code class="language-bash"># 创建pool,分成32个PG，PG是跨主机的
ceph osd pool create $pool_name 32 32
ceph osd pool ls
ceph pg ls-by-pool $pool_name | awk '{print $1,$2,$15}'
ceph osd tree
# 测试使用rados上传文件到ceph
sudo rados put msg1 /var/log/syslog --pool=mypool
ceph osd map mypool msg1
</code></pre>
<h2 id="集群扩容节点移除osd操作">集群扩容节点/移除osd操作</h2>
<h3 id="扩展ceph-mon节点">扩展ceph-mon节点</h3>
<pre><code class="language-bash">mon2# apt install ceph-mon
mon3# apt install ceph-mon
deploy# ceph-deploy mon add  mon2 mon3
</code></pre>
<h3 id="扩展ceph-mgr节点">扩展ceph-mgr节点</h3>
<pre><code class="language-bash">mgr2# apt install ceph-mgr
deploy# ceph-deploy mgr create mgr2
</code></pre>
<h3 id="从服务器中新增移除osd">从服务器中新增/移除osd</h3>
<pre><code class="language-bash">### 新增OSD
方法同上述部署OSD过程。

### 移除osd
deploy # ceph osd out {osd-num}
osd# sudo systemctl stop ceph-osd@{osd-num}
osd# ceph osd purge {id} --yes-i-really-mean-it

</code></pre>
<h1 id="ceph集群应用基础">ceph集群应用基础</h1>
<h2 id="块设备-rbd">块设备 RBD</h2>
<p>RBD（RADOS Block Device ）为块存储的一种，RBD通过librbd库与OSD交互。</p>
<h3 id="创建块设备存储rbd">创建块设备存储RBD</h3>
<pre><code class="language-bash">### 创建存储池
# ceph osd pool create &lt;poolname&gt; pg_num pgp_num {replicated|erasure}

### 创建一个名为myrbd1,pg/pgp为64大小的存储池
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph osd pool create myrbdpool 64 64
pool 'myrbdpool' created
### 对存储池启用rbd功能
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph osd pool application enable myrbdpool rbd
enabled application 'rbd' on pool 'myrbdpool'

### 对存储池初始化
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd pool init -p myrbdpool

### 在RBD存储池中创建2个镜像(myimg2可用于低版本内核挂载测试，仅开启layering特性)
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd create myimg1 --size 2G --pool myrbdpool
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd create myimg2 --size 3G --pool myrbdpool --image-format 2 --image-feature layering

### 查看rbd存储池镜像信息
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd ls --pool myrbdpool
myimg1
myimg2

ceph-admin@ceph-deploy:~/ceph-cluster$ rbd --image myimg1 --pool myrbdpool info
rbd image 'myimg1':
	size 2 GiB in 512 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 127673be7816
	block_name_prefix: rbd_data.127673be7816
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Sun Aug 22 23:18:30 2021
	access_timestamp: Sun Aug 22 23:18:30 2021
	modify_timestamp: Sun Aug 22 23:18:30 2021
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd --image myimg2 --pool myrbdpool info
rbd image 'myimg2':
	size 3 GiB in 768 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 60aab8f8a2bb
	block_name_prefix: rbd_data.60aab8f8a2bb
	format: 2
	features: layering
	op_features:
	flags:
	create_timestamp: Sun Aug 22 23:19:14 2021
	access_timestamp: Sun Aug 22 23:19:14 2021
	modify_timestamp: Sun Aug 22 23:19:14 2021
</code></pre>
<h3 id="客户端admin账户挂载使用块存储">客户端admin账户挂载使用块存储</h3>
<pre><code class="language-bash">### 客户端需要安装ceph-common，并在 /etc/ceph/下存在 ceph.client.admin.keyring 认证文件
ceph-admin@ceph-deploy:/etc/ceph$ ll /etc/ceph/
total 20
drwxr-xr-x    2 root root 4096 Aug 22 22:51 ./
drwxr-xr-x  100 root root 4096 Aug 22 22:22 ../
-rw-rw----+   1 root root  151 Aug 22 22:51 ceph.client.admin.keyring
-rw-r--r--    1 root root  307 Aug 22 22:51 ceph.conf
-rw-r--r--    1 root root   92 Jul  8 22:17 rbdmap
-rw-------    1 root root    0 Aug 22 22:51 tmpW61qMP

### 客户端映射img
#### 挂载myimg1，提示部分特性无法支持(使用内核版本为4.15.0-112)
root@ceph-deploy:~# rbd -p myrbdpool map myimg1
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable myrbdpool/myimg1 object-map fast-diff deep-flatten&quot;.
In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.
rbd: map failed: (6) No such device or address

root@ceph-deploy:~# dmesg | tail
[ 9546.995674] rbd: image myimg1: image uses unsupported features: 0x38
#### 挂载myimg2, 仅开启部分特性，可以挂载
root@ceph-deploy:~# rbd -p myrbdpool map myimg2
/dev/rbd0

### 检查挂载情况
root@ceph-deploy:~# lsblk
NAME                   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                      8:0    0  100G  0 disk
└─sda1                   8:1    0  100G  0 part
  ├─ubu1804--vg-root   253:0    0   99G  0 lvm  /
  └─ubu1804--vg-swap_1 253:1    0  980M  0 lvm  [SWAP]
sr0                     11:0    1  951M  0 rom
rbd0                   252:0    0    3G  0 disk

### 格式化并尝试写入数据
root@ceph-deploy:~# mkfs.ext4 /dev/rbd0
mke2fs 1.44.1 (24-Mar-2018)
Discarding device blocks: done
Creating filesystem with 786432 4k blocks and 196608 inodes
Filesystem UUID: 31f10239-3088-490a-9544-ebda0e5379b0
Superblock backups stored on blocks:
	32768, 98304, 163840, 229376, 294912

Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done

root@ceph-deploy:~# mkdir /cephrbd
root@ceph-deploy:~# mount /dev/rbd0 /cephrbd/
root@ceph-deploy:~# cp /var/log/syslog /cephrbd/
root@ceph-deploy:~# ll /cephrbd/
total 212
drwxr-xr-x  3 root root   4096 Aug 22 23:32 ./
drwxr-xr-x 24 root root   4096 Aug 22 23:31 ../
drwx------  2 root root  16384 Aug 22 23:31 lost+found/
-rw-r-----  1 root root 191373 Aug 22 23:32 syslog
root@ceph-deploy:/cephrbd# dd if=/dev/zero of=/cephrbd/ceph-testfile bs=1MB count=300
300+0 records in
300+0 records out
300000000 bytes (300 MB, 286 MiB) copied, 0.358303 s, 837 MB/s
root@ceph-deploy:/cephrbd# dd if=/dev/zero of=/cephrbd/ceph-testfile2 bs=64K count=3000
3000+0 records in
3000+0 records out
196608000 bytes (197 MB, 188 MiB) copied, 0.113061 s, 1.7 GB/s

root@ceph-deploy:/cephrbd# ls -alh
total 474M
drwxr-xr-x  3 root root 4.0K Aug 22 23:33 .
drwxr-xr-x 24 root root 4.0K Aug 22 23:31 ..
-rw-r--r--  1 root root 287M Aug 22 23:33 ceph-testfile
-rw-r--r--  1 root root 188M Aug 22 23:33 ceph-testfile2
drwx------  2 root root  16K Aug 22 23:31 lost+found
-rw-r-----  1 root root 187K Aug 22 23:32 syslog
</code></pre>
<h3 id="客户端普通账户挂载使用块存储">客户端普通账户挂载使用块存储</h3>
<ol>
<li>创建普通用户，并生成keyring文件</li>
</ol>
<pre><code class="language-bash">### 创建普通用户user1, 并对 myrbdpool 存储池添加权限
ceph-admin@ceph-deploy:~$ ceph auth add client.user1 mon 'allow r' osd 'allow rwx pool=myrbdpool'
added key for client.user1
#### 查看用户信息
ceph-admin@ceph-deploy:~$ ceph auth get client.user1
[client.user1]
	key = AQBmtiNhCKWpFRAApg3x1RvMqHNjU+JCiOhjsQ==
	caps mon = &quot;allow r&quot;
	caps osd = &quot;allow rwx pool=myrbdpool&quot;
exported keyring for client.user1
#### 导出用户秘钥文件
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.user1.keyring
creating ceph.client.user1.keyring
ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 300
drwxrwxr-x 2 ceph-admin ceph-admin   4096 Aug 23 22:54 ./
drwxr-xr-x 6 ceph-admin ceph-admin   4096 Aug 22 23:01 ../
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mds.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mgr.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-osd.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-rgw.keyring
-rw------- 1 ceph-admin ceph-admin    151 Aug 22 22:51 ceph.client.admin.keyring
-rw------- 1 ceph-admin ceph-admin      0 Aug 23 22:54 ceph.client.user1.keyring
-rw-rw-r-- 1 ceph-admin ceph-admin    307 Aug 22 22:37 ceph.conf
-rw-rw-r-- 1 ceph-admin ceph-admin 258705 Aug 22 23:05 ceph-deploy-ceph.log
-rw------- 1 ceph-admin ceph-admin     73 Aug 22 22:37 ceph.mon.keyring
-rwxrwxr-x 1 ceph-admin ceph-admin    897 Aug 22 23:01 ceph_osdcreate.sh*
-rwxrwxr-x 1 ceph-admin ceph-admin    735 Aug 22 23:00 ceph_osdzap.sh*
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph auth get client.user1 -o ceph.client.user1.keyring
exported keyring for client.user1
ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 304
drwxrwxr-x 2 ceph-admin ceph-admin   4096 Aug 23 22:54 ./
drwxr-xr-x 6 ceph-admin ceph-admin   4096 Aug 22 23:01 ../
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mds.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mgr.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-osd.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-rgw.keyring
-rw------- 1 ceph-admin ceph-admin    151 Aug 22 22:51 ceph.client.admin.keyring
-rw------- 1 ceph-admin ceph-admin    124 Aug 23 22:55 ceph.client.user1.keyring
-rw-rw-r-- 1 ceph-admin ceph-admin    307 Aug 22 22:37 ceph.conf
-rw-rw-r-- 1 ceph-admin ceph-admin 258705 Aug 22 23:05 ceph-deploy-ceph.log
-rw------- 1 ceph-admin ceph-admin     73 Aug 22 22:37 ceph.mon.keyring
-rwxrwxr-x 1 ceph-admin ceph-admin    897 Aug 22 23:01 ceph_osdcreate.sh*
-rwxrwxr-x 1 ceph-admin ceph-admin    735 Aug 22 23:00 ceph_osdzap.sh*
ceph-admin@ceph-deploy:~/ceph-cluster$ cat ceph.client.user1.keyring
[client.user1]
	key = AQBmtiNhCKWpFRAApg3x1RvMqHNjU+JCiOhjsQ==
	caps mon = &quot;allow r&quot;
	caps osd = &quot;allow rwx pool=myrbdpool&quot;

</code></pre>
<ol>
<li>在客户端中安装ceph-common客户端（对应Ceph版本）</li>
<li>将用户keyring文件同步到/etc/ceph/目录下</li>
</ol>
<pre><code class="language-bash">ceph-admin@ceph-deploy:~/ceph-cluster$ rsync -avzPr ceph.client.user1.keyring root@ceph-osd4:/etc/ceph/
root@ceph-osd4's password:
sending incremental file list
ceph.client.user1.keyring
            124 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/1)

sent 239 bytes  received 35 bytes  49.82 bytes/sec
total size is 124  speedup is 0.45
</code></pre>
<ol>
<li>在客户端验证权限，并挂载镜像</li>
</ol>
<pre><code class="language-bash">root@ceph-osd4:~# ll /etc/ceph/
total 24
drwxr-xr-x   2 root       root       4096 Aug 23 22:59 ./
drwxr-xr-x  99 root       root       4096 Aug 22 22:59 ../
-rw-rw----+  1 root       root        151 Aug 22 22:52 ceph.client.admin.keyring
-rw-------   1 ceph-admin ceph-admin  124 Aug 23 22:55 ceph.client.user1.keyring
-rw-r--r--   1 root       root        307 Aug 22 23:05 ceph.conf
-rw-r--r--   1 root       root         92 Jul  8 22:17 rbdmap
-rw-------   1 root       root          0 Aug 22 22:52 tmp5Reg6c

### 映射rbd为 /dev/rbd{x}
root@ceph-osd4:/# rbd --user user1 -p myrbdpool map myimg2
/dev/rbd0
rbd: --user is deprecated, use --id
### 如为新创建镜像，则先格式化镜像
root@ceph-osd4:/# mkfs.ext4 /dev/rbd0
### 挂载镜像
root@ceph-osd4:/# mount /dev/rbd0 /ceph-rbd/
root@ceph-osd4:/# cd /ceph-rbd/
root@ceph-osd4:/ceph-rbd# ll
total 485184
drwxr-xr-x  3 root root      4096 Aug 22 23:33 ./
drwxr-xr-x 24 root root      4096 Aug 23 23:00 ../
-rw-r--r--  1 root root 300000000 Aug 22 23:33 ceph-testfile
-rw-r--r--  1 root root 196608000 Aug 22 23:33 ceph-testfile2
drwx------  2 root root     16384 Aug 22 23:31 lost+found/
-rw-r-----  1 root root    191373 Aug 22 23:32 syslog
</code></pre>
<h3 id="扩容rbd镜像">扩容rbd镜像</h3>
<pre><code class="language-bash">### 查看当前镜像大小
root@ceph-deploy:/# rbd ls -p myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  3 GiB            2
### 从3G扩容到5G
root@ceph-deploy:/# rbd resize --pool myrbdpool --image myimg2 --size 5G
Resizing image: 100% complete...done.
root@ceph-deploy:/# rbd ls -p myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  5 GiB            2
### 验证客户端扩容结果
root@ceph-osd4:/ceph-rbd# fdisk -l /dev/rbd0
Disk /dev/rbd0: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes
root@ceph-osd4:/ceph-rbd# df -Th
Filesystem                   Type      Size  Used Avail Use% Mounted on
udev                         devtmpfs  966M     0  966M   0% /dev
tmpfs                        tmpfs     200M  8.9M  191M   5% /run
/dev/mapper/ubu1804--vg-root ext4       97G  2.7G   90G   3% /
tmpfs                        tmpfs     997M     0  997M   0% /dev/shm
tmpfs                        tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs                        tmpfs     997M     0  997M   0% /sys/fs/cgroup
tmpfs                        tmpfs     200M     0  200M   0% /run/user/1000
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-15
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-16
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-17
/dev/rbd0                    ext4      2.9G  483M  2.3G  18% /ceph-rbd

</code></pre>
<h3 id="卸载rbd镜像">卸载RBD镜像</h3>
<pre><code class="language-bash"># umount /ceph-rbd
# rbd --id user1 -p myrbdpool unmap myimg2

</code></pre>
<h3 id="删除rbd镜像">删除RBD镜像</h3>
<pre><code class="language-bash"># rbd rm --pool myrbdpool --image myimg2
</code></pre>
<h3 id="rbd镜像回收站">RBD镜像回收站</h3>
<p>可以将需要删除的镜像先移动到回收站，后期确认删除时再进行永久删除。</p>
<pre><code class="language-bash">### 查看镜像状态
root@ceph-deploy:/# rbd status --pool myrbdpool --image myimg1
Watchers: none
root@ceph-deploy:/# rbd status --pool myrbdpool --image myimg2
Watchers:
	watcher=10.0.11.109:0/3891133225 client.4938 cookie=18446462598732840961
	watcher=10.0.11.100:0/1430785202 client.4789 cookie=18446462598732840962
root@ceph-deploy:/# rbd trash move --pool myrbdpool --image myimg2
root@ceph-deploy:/# rbd trash list --pool myrbdpool
60aab8f8a2bb myimg2
root@ceph-deploy:/# rbd trash restore --pool myrbdpool --image myimg2 --image-id 60aab8f8a2bb
root@ceph-deploy:/# rbd ls --pool myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  5 GiB            2
</code></pre>

            </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://vincentgresham.github.io/post/hello-gridea/">
                  <h3 class="post-title">
                    Hello Gridea
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
