<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://vincentgresham.github.io</id>
    <title>TheOpsCafe</title>
    <updated>2022-01-03T02:46:58.752Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://vincentgresham.github.io"/>
    <link rel="self" href="https://vincentgresham.github.io/atom.xml"/>
    <subtitle>æ¸©æ•…è€ŒçŸ¥æ–°</subtitle>
    <logo>https://vincentgresham.github.io/images/avatar.png</logo>
    <icon>https://vincentgresham.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, TheOpsCafe</rights>
    <entry>
        <title type="html"><![CDATA[Ceph åŸºç¡€åŠCeph Pacificé›†ç¾¤éƒ¨ç½²]]></title>
        <id>https://vincentgresham.github.io/post/ceph-ji-chu-ji-ceph-pacific-ji-qun-bu-shu/</id>
        <link href="https://vincentgresham.github.io/post/ceph-ji-chu-ji-ceph-pacific-ji-qun-bu-shu/">
        </link>
        <updated>2021-09-01T15:26:29.000Z</updated>
        <content type="html"><![CDATA[<h1 id="æœ¬èŠ‚é‡ç‚¹">æœ¬èŠ‚é‡ç‚¹</h1>
<ul>
<li>cephçš„ç»„ä»¶å’ŒåŠŸèƒ½</li>
<li>cephçš„æ•°æ®è¯»å†™æµç¨‹</li>
<li>ä½¿ç”¨ceph-deployéƒ¨ç½²cephé›†ç¾¤</li>
<li>æµ‹è¯•cephçš„rbdä½¿ç”¨</li>
</ul>
<h1 id="æœåŠ¡å™¨åˆå§‹åŒ–">æœåŠ¡å™¨åˆå§‹åŒ–</h1>
<h1 id="åˆ†å¸ƒå¼å­˜å‚¨æ¦‚è¿°">åˆ†å¸ƒå¼å­˜å‚¨æ¦‚è¿°</h1>
<ul>
<li>å•æœºå­˜å‚¨</li>
</ul>
<p>å³å°†ç£ç›˜ç›´æ¥å®‰è£…åˆ°ç‰©ç†æœåŠ¡å™¨ä¸­ï¼Œå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼šç£ç›˜IOé—®é¢˜ã€æ‰©å®¹é—®é¢˜ã€é«˜å¯ç”¨é—®é¢˜</p>
<ul>
<li>å•†ä¸šå­˜å‚¨</li>
</ul>
<p>EMCã€NetAPPã€æˆ´å°”ã€åä¸ºã€æµªæ½®ç­‰å‚å•†çš„å•†ä¸šåŒ–å­˜å‚¨ã€‚</p>
<ul>
<li>åˆ†å¸ƒå¼å­˜å‚¨</li>
</ul>
<p>è½¯ä»¶å®šä¹‰å­˜å‚¨ï¼ˆSoftware Defined Storage, SDSï¼‰ï¼Œä½¿ç”¨åŠ¨æ€ã€æ•æ·å’Œè‡ªåŠ¨åŒ–è½¯ä»¶å®šä¹‰çš„å­˜å‚¨ä»£æ›¿ä½æ•ˆï¼ˆ<s>æ˜‚è´µ</s>ï¼‰çš„ä¸“ç”¨å­˜å‚¨ç¡¬ä»¶ï¼Œä»¥æé«˜æ•ˆç‡å¹¶é™ä½æˆæœ¬ã€‚</p>
<p>å¸¸ç”¨çš„éå•†ä¸šåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿæœ‰ï¼šCephã€TFSï¼ˆTaobao FileSystemï¼‰ã€FastDFSã€MogileFSï¼ˆMemcachedå…¬å¸Dangaå¼€å‘ï¼‰ã€MooseFSï¼ŒGlusterFSï¼ˆè¢«RedHatæ”¶è´­ï¼‰ï¼Œå…³äºä¸Šè¿°åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿçš„ä»‹ç»å¯å‚è€ƒ :</p>
<p><a href="https://segmentfault.com/a/1190000039196722">ç›˜ç‚¹åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨ç³»ç»Ÿ</a></p>
<figure data-type="image" tabindex="1"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9760e40b-2f21-4376-b7af-557ed26dbd6a/%E6%88%AA%E5%B1%8F2021-08-15_%E4%B8%8B%E5%8D%885.47.12.png" alt="æˆªå±2021-08-15 ä¸‹åˆ5.47.12.png" loading="lazy"></figure>
<h3 id="æœ‰çŠ¶æ€é›†ç¾¤æ•°æ®è¯»å†™ç‰¹æ€§">æœ‰çŠ¶æ€é›†ç¾¤æ•°æ®è¯»å†™ç‰¹æ€§</h3>
<p>æ•°æ®åˆ†ä¸ºè¯»æ•°æ®å’Œå†™æ•°æ®ï¼Œè¯»å¯ä»¥åœ¨ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹è¯»ï¼Œä½†å†™åªèƒ½åœ¨ç‰¹å®šçš„èŠ‚ç‚¹å†™ã€‚å¦‚Redisçš„masterã€zookeeperçš„leaderã€MySQLçš„masterç­‰åœºæ™¯</p>
<h2 id="åˆ†å¸ƒå¼å­˜å‚¨çš„æ•°æ®ç‰¹æ€§">åˆ†å¸ƒå¼å­˜å‚¨çš„æ•°æ®ç‰¹æ€§</h2>
<p>æ•°æ®ä¿å­˜æ—¶ï¼Œå¯åˆ†ä¸º&quot;æ•°æ®&quot;å’Œ&quot;å…ƒæ•°æ®&quot;ä¸¤éƒ¨åˆ†ï¼Œå…ƒæ•°æ®æ˜¯æ–‡ä»¶çš„å±æ€§ä¿¡æ¯ï¼Œä»¥HDFSä¸ºä¾‹ï¼ŒName Nodeæä¾›æ–‡ä»¶å…ƒæ•°æ®çš„è·¯ç”±åŠŸèƒ½ï¼Œå‘Šè¯‰åº”ç”¨å»å“ªä¸ªæœåŠ¡å™¨å»è¯·æ±‚æ–‡ä»¶å†…å®¹ã€‚Data Nodeå®é™…å­˜å‚¨è¯—å¥ã€æ‰§è¡Œæ•°æ®çš„è¯»å†™è¯·æ±‚ä»¥åŠæ•°æ®çš„é«˜å¯ç”¨åŠŸèƒ½ã€‚</p>
<figure data-type="image" tabindex="2"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2558338f-9d60-4328-b8df-eb1fe79eb23c/Untitled.png" alt="Untitled" loading="lazy"></figure>
<p>åœ¨éƒ¨ç½²åˆ†å¸ƒå¼å­˜å‚¨æ—¶ï¼Œä¸€èˆ¬ä¼šå°†æ•°æ®åŒæ­¥ç«¯å£å’Œä¸šåŠ¡ç«¯å£åˆ†ç¦»ï¼Œé˜²æ­¢ç½‘ç»œå¸¦å®½/ç¨³å®šæ€§ç“¶é¢ˆã€‚å­˜å‚¨æ•°æ®æ—¶æ˜¯ä¸€ä¸»ä¸¤å¤‡ä»½ï¼Œä»¥å®ç°æ•°æ®çš„é«˜å¯ç”¨ã€‚å‰¯æœ¬æ± ä¸­åŒ…å«3ä¸ªå‰¯æœ¬ï¼Œä¹Ÿå¯ä»¥5ä¸ªå‰¯æœ¬ï¼Œå‰¯æœ¬è¶Šå¤šè¶Šå ç”¨ç©ºé—´ï¼Œä½†æ˜¯æ•°æ®å®‰å…¨æ€§ä¹Ÿæ›´é«˜ï¼Œå‰¯æœ¬æ•°é‡å¤šçš„æƒ…å†µä¸‹ä¼šæœ‰å†™æ”¾å¤§çš„é—®é¢˜ï¼ˆå‰¯æœ¬è¶Šå¤šï¼Œå®ŒæˆåŒæ­¥å†™æ‰€éœ€çš„æ—¶é•¿è¶Šé•¿ï¼‰ã€‚</p>
<h2 id="å­˜å‚¨ç±»å‹">å­˜å‚¨ç±»å‹</h2>
<p>æ ¹æ®å­˜å‚¨çš„ç±»å‹ï¼Œå¯ä»¥åˆ†ä¸ºå—å­˜å‚¨ã€æ–‡ä»¶å­˜å‚¨ã€å¯¹è±¡å­˜å‚¨ï¼š</p>
<ul>
<li>å—å­˜å‚¨ï¼šéœ€è¦æ ¼å¼åŒ–ï¼Œå°†æ–‡ä»¶ç›´æ¥ä¿å­˜åœ¨ç£ç›˜ä¸Šã€‚</li>
<li>æ–‡ä»¶å­˜å‚¨ï¼šæä¾›æ•°æ®å­˜å‚¨çš„æ¥å£ï¼Œç”±æ“ä½œç³»ç»Ÿå¯¹å—å­˜å‚¨æ§åˆ¶ï¼Œç”±æ“ä½œç³»ç»Ÿæä¾›å­˜å‚¨æ¥å£ï¼Œåº”ç”¨ç¨‹åºé€šè¿‡è°ƒç”¨æ“ä½œç³»ç»Ÿå°†æ–‡ä»¶ä¿å­˜åˆ°å—å­˜å‚¨è¿›è¡ŒæŒä¹…åŒ–ã€‚</li>
<li>å¯¹è±¡å­˜å‚¨ï¼šæ–‡ä»¶è¢«æ‹†åˆ†ä¸ºè‹¥å¹²ä¸ªéƒ¨åˆ†åˆ†æ•£åœ¨å¤šä¸ªå­˜å‚¨æœåŠ¡å™¨ã€‚åœ¨å¯¹è±¡å­˜å‚¨ä¸­ï¼Œæ•°æ®è¢«æ‹†è§£ä¸ºæˆä¸ºâ€å¯¹è±¡â€œçš„ç¦»æ•£å•å…ƒï¼Œå¹¶ä¿å­˜åœ¨å•ä¸ªå­˜å‚¨åº“ä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶æˆ–æœåŠ¡å™¨ä¸Šçš„å—æ¥ä¿å­˜ã€‚éœ€è¦ä¸€ä¸ªæ¥å£ç”¨äºä¸Šä¼ ä¸‹è½½ã€‚</li>
</ul>
<h1 id="ceph-ç®€ä»‹">Ceph ç®€ä»‹</h1>
<p>Cephæ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼ŒåŒæ—¶æ”¯æŒå¯¹è±¡å­˜å‚¨ã€å—å­˜å‚¨ã€æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ï¼Œæ‰€ä»¥ä¹Ÿç§°ä¹‹ä¸º ç»Ÿä¸€å­˜å‚¨ã€‚</p>
<p>Cephæ˜¯æ”¯æŒæä¾›å¯¹è±¡å¼ï¼ˆObjectï¼‰å­˜å‚¨ï¼Œå®ƒæŠŠæ¯ä¸€ä¸ªå¾…ç®¡ç†çš„æ•°æ®æµï¼ˆæ–‡ä»¶ç­‰æ•°æ®ï¼‰åˆ‡åˆ†ä¸ºä¸€åˆ°å¤šä¸ªå›ºå®šå¤§å°ï¼ˆé»˜è®¤4Mï¼‰çš„å¯¹è±¡æ•°æ®ï¼Œå¹¶ä»¥å…¶ä¸ºåŸå­å•å…ƒï¼ˆåŸå­æ˜¯æ„æˆå…ƒç´ çš„æœ€å°å•å…ƒï¼‰å®Œæˆæ•°æ®çš„è¯»å†™ã€‚</p>
<p>å¯¹è±¡æ•°æ®çš„åº•å±‚å­˜å‚¨æ˜¯ç”±å¤šä¸ªå­˜å‚¨ä¸»æœºï¼ˆhostï¼‰ç»„æˆçš„å­˜å‚¨é›†ç¾¤ï¼Œè¯¥é›†ç¾¤ä¹Ÿè¢«ç§°ä¹‹ä¸ºRADOS(reliable automatic distributed object store)å­˜å‚¨é›†ç¾¤ï¼Œå³å¯é çš„ã€è‡ªåŠ¨åŒ–çš„ã€åˆ†å¸ƒå¼çš„å¯¹è±¡å­˜å‚¨ã€‚libradosæ˜¯RADOSå­˜å‚¨é›†ç¾¤çš„APIï¼Œæ”¯æŒC/C++/JAVA/Python/Ruby/PHP ç­‰ç¼–ç¨‹è¯­è¨€å®¢æˆ·ç«¯ã€‚</p>
<p>Cephä½¿ç”¨RADOSæä¾›å¯¹è±¡å­˜å‚¨ï¼Œé€šè¿‡libradoså°è£…åº“æä¾›å¤šç§å­˜å‚¨æ–¹å¼çš„æ–‡ä»¶å’Œå¯¹è±¡è½¬æ¢ã€‚å¤–å±‚é€šè¿‡RGWï¼ˆObjectå¯¹è±¡å­˜å‚¨æ¥å£ï¼Œæœ‰åŸç”ŸAPIï¼Œä¹Ÿå…¼å®¹Swiftå’ŒS3çš„APIï¼‰æä¾›å¯¹è±¡å­˜å‚¨æœåŠ¡ã€RBDï¼ˆBlockå—å­˜å‚¨ï¼Œæ”¯æŒç²¾ç®€é…ç½®ã€å¿«ç…§ã€å…‹éš†ï¼Œé€‚åˆå¤šå®¢æˆ·ç«¯æœ‰ç›®å½•ç»“æ„ï¼‰æä¾›å—å­˜å‚¨æœåŠ¡ã€CephFSï¼ˆFileæ–‡ä»¶å­˜å‚¨ï¼ŒPosixæ¥å£ï¼Œæ”¯æŒå¿«ç…§ï¼Œé€‚åˆå˜åŠ¨å°‘çš„æ•°æ®ï¼Œæ²¡æœ‰ç›®å½•ç»“æ„ä¸èƒ½ç›´æ¥æ‰“å¼€ï¼‰æä¾›æ–‡ä»¶å­˜å‚¨æœåŠ¡ã€‚</p>
<figure data-type="image" tabindex="3"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a443a80b-f636-49b0-ace8-53c7c5fe5066/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h2 id="ceph-è®¾è®¡æ€æƒ³">ceph è®¾è®¡æ€æƒ³</h2>
<ul>
<li>æ¯ä¸€ç»„ä»¶çš†å¯æ‰©å±•</li>
<li>æ— å•ç‚¹æ•…éšœ</li>
<li>åŸºäºè½¯ä»¶è€Œä¸”å¼€æºï¼Œæ²¡æœ‰å‚å•†é”å®š</li>
<li>åœ¨ç°æœ‰å»‰ä»·ç¡¬ä»¶ä¸Šè¿è¡Œ</li>
<li>å°½å¯èƒ½è‡ªåŠ¨ç®¡ç†ï¼Œå‡å°‘ç”¨æˆ·å¹²é¢„</li>
</ul>
<h2 id="ceph-ç‰ˆæœ¬">ceph ç‰ˆæœ¬</h2>
<p>Cephæ˜¯åœ£å…‹é²å…¹åŠ åˆ©ç¦å°¼äºšå¤§å­¦çš„Sage Weilåœ¨2003å¹´å¼€å‘çš„ï¼Œä¹Ÿæ˜¯ä»–çš„åšå£«å­¦ä½é¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚åˆå§‹çš„é¡¹ç›®åŸå‹æ˜¯å¤§çº¦40000è¡ŒC++ä»£ç çš„Cephæ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶äº2006å¹´ä½œä¸ºå‚è€ƒå®ç°å’Œç ”ç©¶å¹³å°éµå¾ªLGPLåè®®ï¼ˆLesser GUN Public Licenseï¼‰å¼€æºã€‚ç¾å›½åŠ³ä¼¦æ–¯åˆ©ç‰©è«å›½å®¶å®éªŒå®¤ï¼ˆLawrence Livermore National Laboratoryï¼‰èµ„åŠ©äº†Sageçš„åˆå§‹ç ”ç©¶å·¥ä½œã€‚2003ï½2007å¹´æ˜¯Cephçš„ç ”ç©¶å¼€å‘æ—¶æœŸã€‚åœ¨è¿™æœŸé—´ï¼Œå®ƒçš„æ ¸å¿ƒç»„ä»¶é€æ­¥å½¢æˆï¼Œå¹¶ä¸”ç¤¾åŒºå¯¹é¡¹ç›®çš„è´¡çŒ®ä¹Ÿå·²ç»å¼€å§‹é€æ¸å˜å¤§ã€‚Cephæ²¡æœ‰é‡‡ç”¨åŒé‡è®¸å¯æ¨¡å¼ï¼Œä¹Ÿå°±ä¸å­˜åœ¨åªé’ˆå¯¹ä¼ä¸šç‰ˆçš„ç‰¹æ€§ã€‚</p>
<p>Cephçš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬æ˜¯0.1ï¼Œå‘å¸ƒæ—¥æœŸæ˜¯2008.1ï¼Œå…¶ç‰ˆæœ¬å‘½åéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š</p>
<ul>
<li>x.0.z - å¼€å‘ç‰ˆ ç”¨äºå¼€å‘è€…ã€æ—©æœŸæµ‹è¯•è€…å’Œå‹‡å£«</li>
<li>x.1.z - å€™é€‰ç‰ˆ ç”¨äºæµ‹è¯•é›†ç¾¤ã€é«˜æ‰‹</li>
<li>x.2.z - ç¨³å®šï¼Œä¿®æ­£ç‰ˆ ç”¨æˆ·å‘å¸ƒç»™ç”¨æˆ·</li>
</ul>
<p>xå°†ä»9ç®—èµ·ï¼Œå®ƒä»£è¡¨Infernalisï¼ˆé¦–å­—æ¯Iæ˜¯è‹±æ–‡å•è¯ä¸­çš„ç¬¬9ä¸ªå­—æ¯ï¼‰ï¼Œåˆ™å¯¹åº”çš„å¼€å‘ç‰ˆæœ¬ï¼š</p>
<ul>
<li>9.0.0,9.0.1,9.0.2 å¼€å‘ç‰ˆ</li>
<li>9.1.0,9.1.1,9.1.2 å€™é€‰ç‰ˆ</li>
<li>9.2.0,9.2.1,9.2.2 ç¨³å®šç‰ˆ</li>
</ul>
<p><a href="https://www.notion.so/98fbc4b3058549cbb0ea81b2d2e10529">Active Ceph Releases</a></p>
<h2 id="ceph-æ ¸å¿ƒæœ¯è¯­">Ceph æ ¸å¿ƒæœ¯è¯­</h2>
<ul>
<li>RADOSï¼šReliable Autonomic Distributed Object Store, å¯é çš„ã€è‡ªåŠ¨åŒ–çš„ã€åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ç³»ç»Ÿã€‚RADOSæ˜¯Cephé›†ç¾¤çš„æ ¸å¿ƒï¼Œç”¨äºå®ç°æ•°æ®åˆ†é…ã€æ•…éšœè½¬ç§»æ¢å¤ç­‰é›†ç¾¤æ“ä½œ</li>
<li>libradosï¼šå°†RADOSå°è£…ä¸ºä¸€ä¸ªåº“ï¼Œæ–¹ä¾¿ä¸Šå±‚çš„RBDã€RGWå’ŒCephFSè°ƒç”¨ï¼Œæ”¯æŒPHPã€Rubyã€Javaã€Pythonã€Cå’ŒC++ã€‚</li>
<li>CRUSHï¼šControlled Replication Under Scalable Hashingï¼Œä¸€ç§ç®—æ³•ï¼ŒCephä½¿ç”¨CRUSHç®—æ³•è®¡ç®—å¯¹è±¡å­˜å‚¨ä½ç½®ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„é›†ä¸­å¼å­˜å‚¨å…ƒæ•°æ®å¯»å€æ–¹æ¡ˆã€‚Cephå°†æ•°æ®é€šè¿‡å¯¹è±¡çš„å½¢å¼å­˜å‚¨åœ¨é€»è¾‘å­˜å‚¨æ± ï¼ˆPoolï¼‰ä¸­ï¼Œé€šè¿‡CRUSHç®—æ³•è®¡ç®—å‡ºå“ªä¸€ä¸ªPGåº”è¯¥å­˜æ”¾å¯¹è±¡ï¼Œå¹¶è¿›ä¸€æ­¥è®¡ç®—å‡ºå“ªäº›OSDåº”è¯¥å­˜æ”¾PGã€‚</li>
<li>Poolï¼šå­˜å‚¨å¯¹è±¡çš„é€»è¾‘åˆ†åŒºï¼Œè§„å®šäº†æ•°æ®å†—ä½™çš„ç±»å‹å’Œå¯¹åº”çš„å‰¯æœ¬åˆ†å¸ƒç­–ç•¥ï¼Œæ”¯æŒä¸¤ç§ç±»å‹ï¼šå‰¯æœ¬(replicated)å’Œçº åˆ ç (Erasure Code)ã€‚ä¸€ä¸ªpoolå†…éƒ¨å¯ä»¥æœ‰å¤šä¸ªPGã€‚</li>
<li>PGï¼šPlacement Groupï¼Œæ”¾ç½®ç­–ç•¥ç»„ï¼Œä¸€ç³»åˆ—å¯¹è±¡çš„é›†åˆï¼Œåœ¨è¯¥é›†åˆå†…çš„æ‰€æœ‰å¯¹è±¡éƒ½å…·æœ‰ç›¸åŒçš„æ”¾ç½®ç­–ç•¥ã€‚PGæ˜¯Cephä¸­çš„é€»è¾‘æ¦‚å¿µï¼ŒæœåŠ¡ç«¯æ•°æ®å‡è¡¡å’Œæ¢å¤çš„æœ€å°ç²’åº¦å°±æ˜¯PGï¼Œä¸€ä¸ªPGåŒ…å«å¤šä¸ªOSDã€‚å¼•å…¥PGæ˜¯ä¸ºäº†æ›´å¥½åœ°åˆ†é…å’Œå®šä½æ•°æ®ã€‚</li>
<li>Objectï¼šæœ€åº•å±‚çš„å­˜å‚¨å•å…ƒï¼ŒåŒ…å«å…ƒæ•°æ®å’Œåˆå§‹æ•°æ®ã€‚</li>
</ul>
<p>å…¶ä½™æœ¯è¯­ï¼Œå¯å‚è€ƒ ï¼š</p>
<p><a href="https://docs.ceph.com/en/latest/glossary/"></a></p>
<h2 id="ceph-ç»„ä»¶">Ceph ç»„ä»¶</h2>
<figure data-type="image" tabindex="4"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c904316c-d910-4fba-880b-cf29f15679b8/Untitled.png" alt="Untitled" loading="lazy"></figure>
<p>Ceph ä¸»è¦åŒ…å«å››ä¸ªç»„ä»¶ï¼šMonitorsã€Managersã€Ceph OSDsã€MDSs</p>
<h3 id="ceph-monitorsceph-mon-ç›‘è§†å™¨">Ceph Monitors(ceph-mon) ç›‘è§†å™¨</h3>
<p>Cephé›†ç¾¤çš„ç›‘è§†å™¨ï¼Œä¸€ä¸ªCephé›†ç¾¤éœ€è¦å¤šä¸ªMonitorsç»„æˆçš„å°é›†ç¾¤ï¼Œé€šè¿‡PaxosåŒæ­¥æ•°æ®ï¼Œç”¨äºä¿å­˜OSDçš„å…ƒæ•°æ®ã€‚è´Ÿè´£ç›‘æ§æ•´ä¸ªCephé›†ç¾¤è¿è¡Œæ—¶çš„Mapè§†å›¾ï¼ˆOSD Mapã€Monitor Mapã€PG Mapã€CRUSH Mapï¼‰ã€‚åŒæ—¶è¿˜ç®¡ç†é›†ç¾¤å®¢æˆ·ç«¯è®¤è¯å’Œæˆæƒã€‚é€šå¸¸ä¸€ä¸ªCephé›†ç¾¤ä¸­è‡³å°‘éœ€è¦3ä¸ªMonèŠ‚ç‚¹æ¥ä¿è¯å†—ä½™å’Œé«˜å¯ç”¨ã€‚</p>
<h3 id="managersceph-mgr-ç®¡ç†å™¨">Managers(ceph-mgr) ç®¡ç†å™¨</h3>
<p>Ceph Manager å®ˆæŠ¤è¿›ç¨‹è´Ÿè´£è·Ÿè¸ªè¿è¡Œæ—¶æŒ‡æ ‡å’ŒCephé›†ç¾¤çš„å½“å‰çŠ¶æ€ï¼ŒåŒ…æ‹¬å½“å‰çš„å­˜å‚¨åˆ©ç”¨ç‡ã€æ€§èƒ½æŒ‡æ ‡ã€ç³»ç»Ÿè´Ÿè½½ã€‚åŒæ—¶ä¹Ÿæä¾›åŸºäºPythonç¼–å†™çš„Ceph Dashboard Webå’ŒREST APIï¼Œå¯ä¾›å¤–éƒ¨è°ƒç”¨å¦‚ï¼šcephmetrics,zabbix,prometheusç­‰ã€‚é€šå¸¸è‡³å°‘éœ€è¦2ä¸ªMgrèŠ‚ç‚¹æ¥ä¿è¯é›†ç¾¤å†—ä½™å’Œé«˜å¯ç”¨æ€§ã€‚</p>
<h3 id="ceph-osdsceph-osd">Ceph OSDs(ceph-osd)</h3>
<p>Ceph Object Storage Daemons å¯¹è±¡å­˜å‚¨å®ˆæŠ¤è¿›ç¨‹,è´Ÿè´£ç‰©ç†å­˜å‚¨çš„è¿›ç¨‹ï¼Œä¸€èˆ¬é…ç½®ä¸ºä¸ç£ç›˜ä¸€ä¸€å¯¹åº”ï¼Œä¸€å—ç£ç›˜å¯åŠ¨ä¸€ä¸ªOSDè¿›ç¨‹ã€‚ ç”¨äºå­˜å‚¨æ•°æ®ï¼Œå¤„ç†é›†ç¾¤æ•°æ®å¤åˆ¶ã€æ¢å¤ã€é‡æ–°å¹³è¡¡ï¼Œå¹¶æ£€æŸ¥å…¶ä»–OSDå®ˆæŠ¤ç¨‹åºçš„å¿ƒè·³æ¥å‘Ceph Monå’ŒMGRæä¾›ç›‘æ§ä¿¡æ¯ã€‚é€šå¸¸è‡³å°‘éœ€è¦3ä¸ªOSDèŠ‚ç‚¹æ¥ä¿è¯å†—ä½™å’Œé«˜å¯ç”¨æ€§ã€‚</p>
<ul>
<li>ä¸€ä¸ªPoolé‡Œæœ‰å¾ˆå¤šPG</li>
<li>ä¸€ä¸ªPGé‡ŒåŒ…å«ä¸€å †å¯¹è±¡ï¼Œä¸€ä¸ªå¯¹è±¡åªèƒ½å±äºä¸€ä¸ªPG</li>
<li>PGæœ‰ä¸»ä»ä¹‹åˆ†ï¼Œä¸€ä¸ªPGåˆ†å¸ƒåœ¨ä¸åŒçš„OSDä¸Šï¼ˆä¸‰å‰¯æœ¬ï¼‰</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6a91c06d-cbcd-4e2a-9ffb-4ff7bd102ea5/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h3 id="ceph-mdssceph-mds">Ceph MDSs(ceph-mds)</h3>
<p>Ceph Metadata Server å…ƒæ•°æ®æœåŠ¡å™¨ï¼Œç”¨äºCephæ–‡ä»¶å­˜å‚¨ï¼ˆCeph-FSï¼‰ï¼ŒCephå—å­˜å‚¨å’Œå¯¹è±¡å­˜å‚¨ä¸ä½¿ç”¨MDSã€‚è´Ÿè´£ä¿å­˜æ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼Œç®¡ç†ç›®å½•ç»“æ„ã€‚</p>
<p>å› æ­¤ï¼Œä¸€ä¸ªCephé›†ç¾¤ï¼Œåº”è‡³å°‘åŒ…å«ï¼š</p>
<ul>
<li>è‡³å°‘éœ€è¦ä¸€ä¸ªCeph Monitors ç›‘è§†å™¨ï¼ˆ1ï¼Œ3ï¼Œ5ï¼Œ7...å¥‡æ•°ï¼‰</li>
<li>ä¸¤ä¸ªæˆ–ä»¥ä¸Šçš„Cephç®¡ç†å™¨Managers</li>
<li>3ä¸ªä»¥ä¸ŠCeph OSDï¼ˆå¯¹è±¡å­˜å‚¨å®ˆæŠ¤ç¨‹åºï¼‰</li>
<li>è‹¥éœ€è¦è¿è¡ŒCephæ–‡ä»¶ç³»ç»Ÿå®¢æˆ·ç«¯æ—¶ï¼Œè¿˜éœ€è¦é«˜å¯ç”¨çš„Ceph Metadata Serverï¼ˆæ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®æœåŠ¡å™¨ï¼‰</li>
</ul>
<h2 id="ceph-æ•°æ®å­˜å‚¨è¿‡ç¨‹">Ceph æ•°æ®å­˜å‚¨è¿‡ç¨‹</h2>
<p>Cephé›†ç¾¤éƒ¨ç½²å¥½ä¹‹åï¼Œè¦å…ˆåˆ›å»ºå­˜å‚¨æ± æ‰èƒ½å‘cephå†™å…¥æ•°æ®ï¼Œæ–‡ä»¶åœ¨å‘cephä¿å­˜ä¹‹å‰è¦å…ˆè¿›è¡Œä¸€è‡´æ€§hashè®¡ç®—ï¼Œè®¡ç®—åä¼šæŠŠæ–‡ä»¶ä¿å­˜åœ¨æŸä¸ªå¯¹åº”çš„PGï¼Œæ­¤æ–‡ä»¶ä¸€å®šå±äºæŸä¸ªpoolçš„ä¸€ä¸ªPGï¼Œå†é€šè¿‡PGä¿å­˜åœ¨OSDä¸Šã€‚æ•°æ®å¯¹è±¡åœ¨å†™åˆ°ä¸»OSDä¹‹åå†åŒæ­¥åˆ°ä»OSDä»¥å®ç°æ•°æ®çš„é«˜å¯ç”¨ã€‚</p>
<figure data-type="image" tabindex="6"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bc5eeb52-9f11-4946-b69e-99e183e8bbd9/%E6%88%AA%E5%B1%8F2021-08-15_%E4%B8%8B%E5%8D%887.22.49.png" alt="æˆªå±2021-08-15 ä¸‹åˆ7.22.49.png" loading="lazy"></figure>
<p>ä»¥RBDå—å­˜å‚¨ä¸ºä¾‹ï¼Œå®Œæ•´æ•°æ®å†™å…¥Cephå®Œæ•´æµç¨‹æ˜¯ï¼š</p>
<p><strong>Fileâ†’ (Pool, Object) â†’ (Pool PG) â†’ OSD set â†’ OSD Disk</strong></p>
<ol>
<li>å®¢æˆ·ç«¯ä»monè·å–æœ€æ–°çš„cluster map</li>
<li>å®¢æˆ·ç«¯é€šè¿‡librdbå¯¹äºŒè¿›åˆ¶è¿›è¡Œåˆ†å—ï¼Œæ˜ å°„ä¸ºRADOSèƒ½å¤Ÿå¤„ç†çš„Objectï¼ˆFileâ†’Objectï¼‰
<ul>
<li>oid(object id) = ino(inode number) + ono(object number)</li>
<li>inoä¸ºfileçš„å…ƒæ•°æ®åºåˆ—å·ï¼Œæ˜¯fileçš„å”¯ä¸€idã€‚oidæ˜¯ç”±fileäº§ç”Ÿçš„æŸä¸ªobjectçš„åºå·ï¼Œé»˜è®¤ä»¥4Må¤§å°åˆ‡åˆ†ã€‚</li>
<li>æŒ‰ç…§Objectçš„æœ€å¤§Sizeè¿›è¡Œåˆ‡åˆ†ï¼Œæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š
<ul>
<li>è®©å¤§å°ä¸ä¸€çš„fileå˜æˆæœ€å¤§sizeä¸€è‡´ï¼Œå¯ä»¥è¢«RADOSé«˜æ•ˆç®¡ç†çš„Objectã€‚</li>
<li>å¯¹å•ä¸€fileçš„ä¸²è¡Œå¤„ç†å˜ä¸ºå¯¹å¤šä¸ªObjectçš„å¹¶è¡Œå¤„ç†ã€‚</li>
</ul>
</li>
</ul>
</li>
<li>å®¢æˆ·ç«¯åŸºäºhashç®—æ³•è®¡ç®—å‡ºobjectå°†è¦å­˜å‚¨çš„PGçš„IDï¼ˆObjectâ†’PGï¼‰
<ul>
<li>fileè¢«æ˜ å°„ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªobjectä¹‹åï¼Œå°±éœ€è¦å°†æ¯ä¸ªobjectç‹¬ç«‹åœ°æ˜ å°„åˆ°ä¸€ä¸ªPGä¸­ã€‚éœ€è¦é€šè¿‡hashç®—æ³•è®¡ç®—å‡ºPGçš„idå·ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šPG_ID = hash(pool_id).hash(object_id) % PG æ€»æ•°
<ol>
<li>å®¢æˆ·ç«¯è¾“å…¥pool IDå’Œobject IDï¼ˆæ¯”å¦‚pool=&quot;mypool&quot;, object_id = &quot;myfile&quot;)</li>
<li>cephå¯¹object ID åšå“ˆå¸Œ</li>
<li>cephå¯¹è¯¥hashå€¼å–PGæ€»æ•°çš„æ¨¡ï¼Œå¾—åˆ°PGç¼–å·ï¼Œæ¯”å¦‚50ï¼ˆç¬¬2ã€3æ­¥åŸºæœ¬ä¿è¯ä¸€ä¸ªpoolä¸­çš„æ‰€æœ‰PGä¼šè¢«å‡åŒ€åœ°ä½¿ç”¨ï¼‰</li>
<li>cephå¯¹pool IDå–hashï¼ˆæ¯”å¦‚ &quot;mypool&quot; = 1ï¼‰</li>
<li>ceph å°†pool IDå’ŒPG IDç»„åˆåœ¨ä¸€èµ·ï¼ˆæ¯”å¦‚1.50ï¼‰å¾—åˆ°PGçš„å®Œæ•´IDã€‚</li>
</ol>
</li>
<li>æ ¹æ®RADOSçš„è®¾è®¡ï¼Œä¸Šè¿°è®¡ç®—æ•´ä½“ç»“æœæ˜¯ä»æ‰€æœ‰mä¸ªPGä¸­è¿‘ä¼¼å‡åŒ€åœ°éšæœºé€‰æ‹©ä¸€ä¸ªã€‚åŸºäºè¿™ä¸ªæœºåˆ¶ï¼Œå½“æœ‰å¤§é‡objectå’Œå¤§é‡PGæ—¶ï¼ŒRADOSèƒ½ä¿è¯objectå’ŒPGä¹‹é—´çš„è¿‘ä¼¼å‡åŒ€æ˜ å°„ã€‚</li>
</ul>
</li>
<li>å®¢æˆ·ç«¯æ ¹æ®CRUSHç®—æ³•è®¡ç®—å‡ºPGä¸­ç›®æ ‡çš„ä¸»/æ¬¡ OSDçš„ID ï¼ˆPG â†’ OSDï¼‰
<ul>
<li>å°†objectçš„é€»è¾‘ç»„ç»‡å•å…ƒçš„PGæ˜ å°„åˆ°æ•°æ®çš„å®é™…å­˜å‚¨å•å…ƒOSD</li>
<li>åœ¨åˆ›å»ºpoolæ—¶ï¼ŒPGä¸OSDçš„å¯¹åº”å…³ç³»å°±ç¡®å®šäº†ï¼Œè¿™é‡Œåªæ˜¯CRUSHæŸ¥æ‰¾ã€‚</li>
<li>è®¡ç®—å…¬å¼ï¼šOSD_IDS=CRUSH(PG_ID, cluster_map, crush_rules)</li>
<li>è®¡ç®—å¾—åˆ°çš„OSD_IDSåŒ…å«nä¸ªOSDï¼Œè¿™äº›OSDså…±åŒè´Ÿè´£å­˜å‚¨å’Œç»´æŠ¤ä¸€ä¸ªPGä¸­çš„æ‰€æœ‰Objectã€‚nçš„æ•°å€¼å¯ä»¥æ ¹æ®å®é™…åº”ç”¨ä¸­å¯¹äºå¯é æ€§çš„éœ€æ±‚è€Œé…ç½®ï¼Œç”Ÿäº§ç¯å¢ƒä¸‹é€šå¸¸ä¸º3ã€‚</li>
<li>å…·ä½“åˆ°æ¯ä¸ªOSDï¼Œåˆ™ç”±å¯¹åº”OSD Daemonè´Ÿè´£æ‰§è¡Œæ˜ å°„åˆ°æœ¬åœ°çš„objectåœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­è¿›è¡Œå­˜å‚¨ã€è®¿é—®ã€å…ƒæ•°æ®ç»´æŠ¤ç­‰æ“ä½œã€‚</li>
</ul>
</li>
</ol>
<p>å°†ä¸Šè¿°å®Œæ•´æµç¨‹æ¦‚è¿°ä¸ºå¦‚ä¸‹ï¼š</p>
<ol>
<li>å®¢æˆ·ç«¯éœ€è¦å‘cephä¸­å†™å…¥ä¸€ä¸ªfileï¼Œé¦–å…ˆåœ¨æœ¬åœ°å®Œæˆå¯»å€æµç¨‹ï¼Œå°†fileå˜æˆnä¸ªobjectï¼Œç„¶åæ‰¾å‡ºå­˜å‚¨æ¯ä¸ªobjectçš„å¯¹åº”ä¸€ç»„ä¸‰ä¸ªOSDã€‚</li>
<li>æ‰¾å‡ºä¸‰ä¸ªOSDåï¼Œå®¢æˆ·ç«¯å°†ç›´æ¥å’ŒPrimary OSDé€šä¿¡ï¼Œå‘èµ·å†™å…¥è¯·æ±‚ã€‚Primary OSDæ”¶åˆ°è¯·æ±‚åï¼Œåˆ†åˆ«å‘Secondary OSDå’ŒTertiary OSDå‘èµ·å†™å…¥æ“ä½œã€‚Secondary/Tertiary OSDå„è‡ªå®Œæˆå†™å…¥æ“ä½œåï¼Œå‘é€ACKç¡®è®¤ä¿¡æ¯ç»™Primary OSDã€‚å½“Primary OSDç¡®è®¤å…¶ä»–ä¸¤ä¸ªOSDå†™å…¥å®Œæˆåï¼Œåˆ™è®¤ä¸ºå½“å‰æ•°æ®å®Œå…¨å†™å…¥ï¼Œå‘å®¢æˆ·ç«¯å‘é€ç¡®è®¤ä¿¡æ¯ï¼Œobjectå†™å…¥å®Œæˆã€‚</li>
</ol>
<figure data-type="image" tabindex="7"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8297bb3f-bc53-4ba7-abf4-e228c13d4ee8/Untitled.png" alt="Untitled" loading="lazy"></figure>
<h2 id="ceph-å…ƒæ•°æ®ä¿å­˜æ–¹å¼">Ceph å…ƒæ•°æ®ä¿å­˜æ–¹å¼</h2>
<p>Cephå¯¹è±¡æ•°æ®çš„å…ƒæ•°æ®ä»¥KVçš„å½¢å¼å­˜åœ¨ï¼Œåœ¨RADOSä¸­æœ‰ä»¥ä¸‹ä¸¤ç§å®ç°ï¼š</p>
<ul>
<li>
<p>xattrs æ‰©å±•å±æ€§ï¼šå°†å…ƒæ•°æ®ä¿å­˜åœ¨æ–‡ä»¶çš„æ‰©å±•å±æ€§ä¸­å¹¶ä¿å­˜åœ¨ç³»ç»Ÿç£ç›˜ä¸Šï¼Œè¦æ±‚æ”¯æŒå¯¹è±¡å­˜å‚¨çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ”¯æŒæ‰©å±•å±æ€§ï¼ˆä¸€èˆ¬ä¸ºXFSï¼‰</p>
</li>
<li>
<p>omap å¯¹è±¡æ˜ å°„ï¼ˆKVï¼‰ï¼šObject Map å¯¹è±¡æ˜ å°„è¡¨ï¼Œå°†å…ƒæ•°æ®ä¿å­˜åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¹‹å¤–ç‹¬ç«‹çš„KVå­˜å‚¨ç³»ç»Ÿä¸­ã€‚</p>
<ul>
<li>filestoreä¸leveldbï¼ˆgoogleå¼€æºï¼‰</li>
<li>bluestoreä¸rocksdb ï¼ˆfacebookå¼€æºï¼‰ï¼šå½“å‰å¸¸ç”¨</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7083d39d-8294-454c-8502-e8f62f0f0140/Untitled.png" alt="Untitled" loading="lazy"></figure>
</li>
</ul>
<p>cephåç«¯å¯ä»¥é€‰æ‹©å¤šç§å­˜å‚¨å¼•æ“ï¼Œæ¯”å¦‚filestoreã€bluestoreç­‰å­˜å‚¨å¯¹è±¡æ•°æ®çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚</p>
<p>å‚è€ƒæ–‡æ¡£ï¼š</p>
<p><a href="https://zhuanlan.zhihu.com/p/41909971">Cephè¯»å†™æ€§èƒ½ä¼°ç®—æ–¹æ³•</a></p>
<h2 id="ceph-crush-ç®—æ³•">Ceph CRUSH ç®—æ³•</h2>
<p>Controller replication under scalable hashingï¼Œä¸€ç§åˆ†å¸ƒå¼ç®—æ³•ï¼Œå®æ—¶è®¡ç®—ï¼Œåœ¨monä¸Šè¿ç®—ã€‚</p>
<p>è‹¥å°†å¯¹è±¡ç›´æ¥æ˜ å°„åˆ°OSDä¸Šä¼šå¯¼è‡´å¯¹è±¡ä¸OSDä¹‹é—´çš„å¯¹åº”å…³ç³»è¿‡äºç´§å¯†å’Œå¶å°”ï¼ŒOSDæ•…éšœæ—¶ä¼šå¯¹æ•´ä¸ªcephé›†ç¾¤äº§ç”Ÿå½±å“ã€‚å› æ­¤cephå°†ä¸€ä¸ªå¯¹è±¡æ˜ å°„åˆ°RADOSé›†ç¾¤æ—¶ï¼š</p>
<ul>
<li>é¦–å…ˆä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•å°†å¯¹è±¡åç§°æ˜ å°„åˆ°PG</li>
<li>å°†PG IDåŸºäºCRUSHç®—æ³•æ˜ å°„åˆ°OSD</li>
</ul>
<p>ä¸Šè¿°ä¸¤æ­¥æ“ä½œéƒ½æ˜¯ä»¥â€œå®æ—¶è®¡ç®—â€çš„æ–¹å¼å®Œæˆï¼Œæœ‰æ•ˆé¿å…äº†ç»„ä»¶â€œä¸­å¿ƒåŒ–â€çš„é—®é¢˜ï¼Œä¹Ÿè§£å†³äº†æŸ¥è¯¢æ€§èƒ½å’Œå†—ä½™é—®é¢˜ï¼Œä½¿cephé›†ç¾¤æ‰©å±•ä¸å†å—æŸ¥è¯¢çš„æ€§èƒ½é™åˆ¶ã€‚</p>
<h1 id="éƒ¨ç½²ceph-é›†ç¾¤">éƒ¨ç½²Ceph é›†ç¾¤</h1>
<p>åœ¨ä¼ä¸šç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œå•å°å¯¹åº”è§’è‰²ä¸»æœºè‡³å°‘éœ€è¦ä»¥ä¸‹é…ç½®ï¼š</p>
<ul>
<li>ceph-monï¼š16C 16G 200G</li>
<li>ceph-mgrï¼š32C 32G 200G ï¼ˆå¯ç”¨å—å­˜å‚¨å’Œå¯¹è±¡å­˜å‚¨ï¼‰</li>
<li>ceph-osdï¼šä½¿ç”¨ä¼ä¸šçº§ssdï¼Œå¤šå—PCIEä¸‡å…†ç½‘å¡</li>
</ul>
<h2 id="æœåŠ¡å™¨åˆå§‹åŒ–-2">æœåŠ¡å™¨åˆå§‹åŒ–</h2>
<p>éœ€è¦å…ˆå®Œæˆä»¥ä¸‹æ“ä½œï¼š</p>
<ul>
<li>å…³é—­selinuxå’Œé˜²ç«å¢™</li>
<li>é…ç½®å¥½æ—¶é—´åŒæ­¥</li>
</ul>
<pre><code class="language-bash">*/5 * * * *  /usr/sbin/ntpdate cn.pool.ntp.org &gt;/dev/null 2&gt;&amp;1
</code></pre>
<ul>
<li>é…ç½®åŸŸåè§£ææˆ–DNSè§£æï¼Œå°†æ‰€æœ‰èŠ‚ç‚¹çš„hostnameã€fqdnå†™å…¥hostsæ–‡ä»¶</li>
<li>é…ç½®å…å¯†ç™»å½•</li>
</ul>
<pre><code class="language-bash">### ä½¿ç”¨Ubuntu 18.04 ç‰ˆæœ¬éƒ¨ç½²
### ä¿®æ”¹å®‰è£…æ—¶çš„å†…æ ¸å‚æ•°ï¼ˆF6ï¼‰ï¼Œä½¿ç½‘å¡åç§°ä¿®æ”¹ä¸ºeth0
net.ifnames=0 biosdevname=0

### å·²å®‰è£…æ“ä½œç³»ç»Ÿä¿®æ”¹ä¿®æ”¹ç½‘å¡åç§°ä¸ºeth*
ops@ubuntu1804:~# sudo vim /etc/default/grub
GRUB_DEFAULT=0
GRUB_TIMEOUT_STYLE=hidden
GRUB_TIMEOUT=2
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;net.ifnames=0 biosdevname=0&quot;
GRUB_CMDLINE_LINUX=&quot;&quot;
ops@ubuntu1804:~# sudo update-grub

### å¦‚æœå·²ä¿®æ”¹è¿‡ç½‘å¡é…ç½®ï¼Œåˆ™éœ€è¦æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„ç½‘å¡å
ops@ubuntu1804:~# vim /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    eth0:
      dhcp4: no
      addresses: [10.0.11.102/16]
      optional: true
      gateway4: 10.0.1.1
      nameservers:
        addresses: [114.114.114.114,8.8.8.8]
    eth1:
      dhcp4: no
      addresses: [192.168.0.102/24]
      optional: true
      nameservers:
        addresses: [114.114.114.114,8.8.8.8]

### é…ç½®å…è®¸rootç™»å½•ï¼Œå¹¶ä¸ºrootç”¨æˆ·è®¾ç½®å¯†ç 
# vim /etc/ssh/sshd_config
PermitRootLogin yes
# passwd
# systemctl restart sshd.service

### å®‰è£…å¸¸ç”¨è½¯ä»¶åŒ…
# apt install iproute2 ntpdate tcpdump telnet traceroute nfs-kernel-server nfs-common lrzsz tree openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev ntpdate tcpdump telnet traceroute gcc openssh-server lrzsz tree openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev ntpdate tcpdump telnet traceroute iotop unzip zip -y

### éƒ¨ç½²æ—¶é—´åŒæ­¥æœåŠ¡
# apt install -y chrony
### ä¿®æ”¹ ceph-deploy èŠ‚ç‚¹chronyé…ç½®ï¼Œä½¿å…¶ä½œä¸ºntp server
# vim /etc/chrony/chrony.conf
allow 192.168.0.0/24
### ä¿®æ”¹æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹é…ç½®ï¼Œä½¿ntp serveræŒ‡å‘ceph-deployèŠ‚ç‚¹
# vim /etc/chrony/chrony.conf
	pool 192.168.0.100        iburst maxsources 4
	#pool ntp.ubuntu.com        iburst maxsources 4
	#pool 0.ubuntu.pool.ntp.org iburst maxsources 1
	#pool 1.ubuntu.pool.ntp.org iburst maxsources 1
	#pool 2.ubuntu.pool.ntp.org iburst maxsources 2

# systemctl restart chrony
# timedatectl set-timezone Asia/Shanghai
# chronyc -a makestep
### éªŒè¯æ—¶é—´åŒæ­¥é…ç½®
# chronyc sources -v

ops@ubuntu1804:~# sudo reboot

</code></pre>
<p>åœ¨å¦‚ä¸‹æµ‹è¯•ç¯å¢ƒä¸­ï¼Œä½¿ç”¨Ubuntu 18.04 LTSéƒ¨ç½²ï¼ŒåŒ…å«1ä¸ªceph-deployèŠ‚ç‚¹ç”¨äºéƒ¨ç½²cephï¼Œ2ä¸ªceph-mgrèŠ‚ç‚¹ï¼Œ3ä¸ªceph-monèŠ‚ç‚¹ï¼Œ4ä¸ªceph-osdèŠ‚ç‚¹ã€‚</p>
<p><a href="https://www.notion.so/d4d2b4fc450845f0ac0a3933a0a85577">Ceph Cluster IPä¿¡æ¯</a></p>
<h3 id="ceph-é›†ç¾¤éƒ¨ç½²ceph-deploy">Ceph é›†ç¾¤éƒ¨ç½²(ceph-deploy)</h3>
<ol>
<li>æŒ‰ç…§æœåŠ¡å™¨åˆå§‹åŒ–æ­¥éª¤ï¼Œé…ç½®å¥½æœåŠ¡å™¨åŸºç¡€ç¯å¢ƒå’ŒIPã€‚</li>
<li>åœ¨æ‰€æœ‰cephèŠ‚ç‚¹ä¸­åˆ›å»ºæ™®é€šç”¨æˆ·ceph-adminï¼Œç”¨äºç®¡ç†cephï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨ç³»ç»Ÿæœ¬èº«çš„éç‰¹æƒè´¦å·ï¼‰ã€‚åœ¨éƒ¨ç½²cephæ—¶ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºåä¸ºcephè´¦å·ï¼Œceph-adminè´¦å·ç”¨äºéƒ¨ç½²å’Œç®¡ç†cephã€‚</li>
</ol>
<pre><code class="language-bash">all-node# groupadd -r -g 2023 ceph-admin &amp;&amp; useradd -r -m -s /bin/bash -u 2023 -g 2023 ceph-admin &amp;&amp; echo ceph-admin:1s1sljj | chpasswd
${ä¸€ä¸ªå¯†ç }

all-node# usermod -aG sudo ceph-admin

### é…ç½®ceph-adminè´¦æˆ·å…å¯†ç™»å½•å…¶ä½™cephèŠ‚ç‚¹
ceph-deploy# su - ceph-admin
ceph-deploy# ssh-keygen -t rsa
ceph-deploy# ssh-copy-id ceph-admin@ceph-deploy
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon1
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon2
ceph-deploy# ssh-copy-id ceph-admin@ceph-mon3
ceph-deploy# ssh-copy-id ceph-admin@ceph-mgr1
ceph-deploy# ssh-copy-id ceph-admin@ceph-mgr2
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd1
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd2
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd3
ceph-deploy# ssh-copy-id ceph-admin@ceph-osd4

### åœ¨æ‰€æœ‰cephèŠ‚ç‚¹ä¸­éƒ¨ç½²python2 
ssh ceph-admin@ceph-mon1 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;
ssh ceph-admin@ceph-mon2 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;
...
ssh ceph-admin@ceph-osd4 &quot;sudo apt install python2.7 -y &amp;&amp; sudo ln -sv /usr/bin/python2.7 /usr/bin/python2&quot;

### åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸­éƒ¨ç½² ceph-common 16.2.5-1bionicç»„ä»¶
ssh ceph-admin@ceph-mon1 &quot;sudo apt install ceph-common -y&quot;
ssh ceph-admin@ceph-mon2 &quot;sudo apt install ceph-common -y&quot;
...
ssh ceph-admin@ceph-osd4 &quot;sudo apt install ceph-common -y&quot;

### åœ¨æ·»åŠ osdä¹‹å‰ï¼Œéƒ¨ç½²osdèŠ‚ç‚¹çš„åŸºæœ¬ç¯å¢ƒï¼Œæ·»åŠ Pç‰ˆçš„æºé…ç½®ï¼ˆæ¥æºï¼šhttps://mirrors.tuna.tsinghua.edu.cn/help/cephï¼‰* ï¼ˆé‡è¦ï¼Œå¦åˆ™å¯èƒ½ç”±äºaptæºä¸­çš„cephç»„ä»¶ç‰ˆæœ¬ä¸åŒè€Œå¯¼è‡´é›†ç¾¤åˆ›å»ºå¤±è´¥ï¼‰

ceph-admin@ceph-deploy:/etc/ceph$ for i in ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
do
ssh ceph-admin@$i &quot;wget -q -O- 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc' | sudo apt-key add -&quot;
ssh ceph-admin@$i &quot;echo 'deb https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic main' | sudo tee -a /etc/apt/sources.list&quot;
ssh ceph-admin@$i &quot;sudo apt-get update&quot;
done
</code></pre>
<ol>
<li>ä½¿ç”¨ceph-deployéƒ¨ç½²ceph</li>
</ol>
<pre><code class="language-bash">ceph-deploy:~# apt-cache madison ceph-deploy
ceph-deploy |      2.0.1 | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main amd64 Packages
ceph-deploy |      2.0.1 | https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic/main i386 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe amd64 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe i386 Packages
ceph-deploy | 1.5.38-0ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/universe Sources
ceph-deploy:~# sudo apt install ceph-deploy
ceph-deploy:~# sudo apt install ceph-common # ç”¨äºç®¡ç†cephï¼Œæ‰§è¡Œceph -s
</code></pre>
<ol>
<li>åˆå§‹åŒ–éƒ¨ç½²mon</li>
</ol>
<pre><code class="language-bash">ops@ceph-deploy:~$ ceph-deploy --help

### å„ä¸ªcephèŠ‚ç‚¹ä¸Šéƒ¨ç½²python2.7
mon:~# apt install python2.7 -y
mon:~# ln -sv /usr/bin/python2.7 /usr/bin/python2

### éƒ¨ç½²æ–°monèŠ‚ç‚¹
ceph-admin@ceph-deploy:~$ mkdir ceph-cluster &amp;&amp; cd $_

ceph-admin@ceph-deploy:~$ ceph-deploy new --cluster-network 192.168.0.0/24 --public-network 10.0.11.0/16 ceph-mon1 ceph-mon2 ceph-mon3

ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 20
drwxrwxr-x 2 ceph-admin ceph-admin 4096 Aug 16 10:47 ./
drwxr-xr-x 6 ceph-admin ceph-admin 4096 Aug 16 10:47 ../
-rw-rw-r-- 1 ceph-admin ceph-admin  261 Aug 16 10:47 ceph.conf  ## è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶
-rw-rw-r-- 1 ceph-admin ceph-admin 3327 Aug 16 10:47 ceph-deploy-ceph.log  ## åˆå§‹åŒ–æ—¥å¿—
-rw------- 1 ceph-admin ceph-admin   73 Aug 16 10:47 ceph.mon.keyring  ## ç”¨äºceph monèŠ‚ç‚¹å†…éƒ¨é€šè®¯è®¤è¯çš„ç§˜é’¥ç¯æ–‡ä»¶

ceph-admin@ceph-deploy:~/ceph-cluster$ cat ceph.conf
[global]
fsid = cb440925-c0f5-49a1-a9f2-ae9470ccd17e
public_network = 10.0.11.0/16
cluster_network = 192.168.0.0/24
mon_initial_members = ceph-mon1, ceph-mon2, ceph-mon3
mon_host = 10.0.11.101,10.0.11.102,10.0.11.103
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

### åœ¨monèŠ‚ç‚¹å®‰è£…ç»„ä»¶ceph-mon 16.2.5-1bionicï¼Œå¹¶åˆå§‹åŒ–monèŠ‚ç‚¹
ceph-mon1 # apt install ceph-mon=16.2.5-1bionic -y
ceph-mon2 # apt install ceph-mon=16.2.5-1bionic -y
ceph-mon3 # apt install ceph-mon=16.2.5-1bionic -y

### åˆå§‹åŒ–monèŠ‚ç‚¹,å°†ä¼šè¯»å–ceph.confä¸­monèŠ‚ç‚¹é…ç½®ï¼Œåˆå§‹åŒ–mon
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy mon create-initial

### æ£€æŸ¥mon daemonæ˜¯å¦å·²è¿è¡Œ
root@ceph-mon1:~# ps -ef|grep ceph-mon
ceph      7192     1  0 10:51 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-mon1 --setuser ceph --setgroup ceph
root      7892  3440  0 10:51 pts/0    00:00:00 grep --color=auto ceph-mon

##  æ¨é€cephé…ç½®å’Œkeyringåˆ°cephä¸­å…¶ä»–èŠ‚ç‚¹,å³å¯ä»¥ä½¿ç”¨å…¶ä»–èŠ‚ç‚¹ç®¡ç†ceph
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy admin ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4

### æ£€æŸ¥cephé›†ç¾¤çŠ¶æ€ï¼ŒæŠ¥é”™æ˜¯ç”±äºceph-adminç”¨æˆ·æ²¡æœ‰æƒé™è¯» /etc/ceph/ è·¯å¾„ä¸‹çš„æ–‡ä»¶
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
2021-08-16T10:52:47.568-0400 7fc6f204c700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory

### ä¿®å¤è¯¥é—®é¢˜ï¼Œåœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ·»åŠ ceph-adminè´¦å·çš„è¯»å†™æƒé™
ceph-admin@ceph-deploy:/etc/ceph$ for i in ceph-deploy ceph-mon1 ceph-mon2 ceph-mon3 ceph-mgr1 ceph-mgr2 ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
do
ssh ceph-admin@$i &quot;sudo setfacl -m u:ceph-admin:rw /etc/ceph/ceph.client.admin.keyring&quot;
done

### æ£€æŸ¥cephé›†ç¾¤çŠ¶æ€
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_WARN
            mons are allowing insecure global_id reclaim

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 40s)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

### è§£å†³ WARN æŠ¥é”™,ç¦ç”¨éå®‰å…¨æ¨¡å¼é€šä¿¡
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph config set mon auth_allow_insecure_global_id_reclaim false

ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 2m)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

</code></pre>
<ol>
<li>åˆå§‹åŒ–éƒ¨ç½²mgr</li>
</ol>
<pre><code class="language-bash">### éƒ¨ç½²mgræœåŠ¡
ceph-admin@ceph-mgr1:~$ sudo apt install ceph-mgr=16.2.5-1bionic -y
ceph-admin@ceph-mgr2:~$ sudo apt install ceph-mgr=16.2.5-1bionic -y

ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy mgr create ceph-mgr1 ceph-mgr2

### éªŒè¯mgræœåŠ¡
ceph-admin@ceph-mgr1:~$ ps -ef | grep mgr
ceph      8745     1  0 11:15 ?        00:00:00 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-mgr1 --setuser ceph --setgroup ceph
ceph-ad+  8815  6069  0 11:16 pts/0    00:00:00 grep --color=auto mgr
</code></pre>
<ol>
<li>åˆå§‹åŒ–éƒ¨ç½²osd</li>
</ol>
<pre><code class="language-bash">~~### éƒ¨ç½²ceph-osdèŠ‚ç‚¹ï¼Œå°†æŒ‰ç…§ä¸²è¡Œä¾æ¬¡å®‰è£…ceph-commonã€ceph-radosgw ç­‰åŸºæœ¬ç»„ä»¶
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy install --no-adjust-repos --nogpgcheck ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4
[ceph-osd4][INFO  ] Running command: sudo ceph --version
[ceph-osd4][DEBUG ] ceph version 16.2.5 (0883bdea7337b95e4b611c768c0279868462204a) pacific (stable)~~

### åˆå§‹åŒ–osdèŠ‚ç‚¹, å°†ä¼šå®‰è£…ceph-common, ceph-radosgw, ceph-osdç­‰ç»„ä»¶ï¼Œç‰ˆæœ¬éœ€å‡ä¸º 16.2.5-1bionic
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy install --release pacific ceph-osd1 ceph-osd2 ceph-osd3 ceph-osd4

# åˆ—å‡ºnodeèŠ‚ç‚¹ä¸Šçš„ç£ç›˜
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy disk list ceph-osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy disk list ceph-osd1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  debug                         : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa41b24afa0&gt;
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['ceph-osd1']
[ceph_deploy.cli][INFO  ]  func                          : &lt;function disk at 0x7fa41b2262d0&gt;
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph-osd1][DEBUG ] connection detected need for sudo
[ceph-osd1][DEBUG ] connected to host: ceph-osd1
[ceph-osd1][DEBUG ] detect platform information from remote host
[ceph-osd1][DEBUG ] detect machine type
[ceph-osd1][DEBUG ] find the location of an executable
[ceph-osd1][INFO  ] Running command: sudo fdisk -l
[ceph-osd1][INFO  ] Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors
[ceph-osd1][INFO  ] Disk /dev/sdb: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdc: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdd: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sdf: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/sde: 16 GiB, 17179869184 bytes, 33554432 sectors
[ceph-osd1][INFO  ] Disk /dev/mapper/ubu1804--vg-root: 99 GiB, 106342383616 bytes, 207699968 sectors
[ceph-osd1][INFO  ] Disk /dev/mapper/ubu1804--vg-swap_1: 980 MiB, 1027604480 bytes, 2007040 sectors

### æ“¦é™¤osdç£ç›˜(æ ¹æ®osdèŠ‚ç‚¹å®é™…ç£ç›˜è¿›è¡Œæ“¦é™¤)
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-deploy disk zap $ceph-node $/dev/sdb
### ceph_osdzap.sh
#!/bin/bash
ceph-deploy disk zap ceph-osd1 /dev/sdb
ceph-deploy disk zap ceph-osd1 /dev/sdc
ceph-deploy disk zap ceph-osd1 /dev/sdd
ceph-deploy disk zap ceph-osd1 /dev/sde
ceph-deploy disk zap ceph-osd1 /dev/sdf

ceph-deploy disk zap ceph-osd2 /dev/sdb
ceph-deploy disk zap ceph-osd2 /dev/sdc
ceph-deploy disk zap ceph-osd2 /dev/sdd
ceph-deploy disk zap ceph-osd2 /dev/sde
ceph-deploy disk zap ceph-osd2 /dev/sdf

ceph-deploy disk zap ceph-osd3 /dev/sdb
ceph-deploy disk zap ceph-osd3 /dev/sdc
ceph-deploy disk zap ceph-osd3 /dev/sdd
ceph-deploy disk zap ceph-osd3 /dev/sde
ceph-deploy disk zap ceph-osd3 /dev/sdf

ceph-deploy disk zap ceph-osd4 /dev/sdb
ceph-deploy disk zap ceph-osd4 /dev/sdc
ceph-deploy disk zap ceph-osd4 /dev/sdd

### æ·»åŠ osd, osd id å°†ä»0å¼€å§‹ï¼Œä¸€å—ç£ç›˜å¯¹åº”ä¸€ä¸ªosd
ceph-admin@ceph-deploy:~/ceph-cluster$  ceph-deploy osd create $ceph-node --data $/dev/sdb...
### ceph_osdcreate.sh
#!/bin/bash
ceph-deploy osd create ceph-osd1 --data /dev/sdb
ceph-deploy osd create ceph-osd1 --data /dev/sdc
ceph-deploy osd create ceph-osd1 --data /dev/sdd
ceph-deploy osd create ceph-osd1 --data /dev/sde
ceph-deploy osd create ceph-osd1 --data /dev/sdf

ceph-deploy osd create ceph-osd2 --data /dev/sdb
ceph-deploy osd create ceph-osd2 --data /dev/sdc
ceph-deploy osd create ceph-osd2 --data /dev/sdd
ceph-deploy osd create ceph-osd2 --data /dev/sde
ceph-deploy osd create ceph-osd2 --data /dev/sdf

ceph-deploy osd create ceph-osd3 --data /dev/sdb
ceph-deploy osd create ceph-osd3 --data /dev/sdc
ceph-deploy osd create ceph-osd3 --data /dev/sdd
ceph-deploy osd create ceph-osd3 --data /dev/sde
ceph-deploy osd create ceph-osd3 --data /dev/sdf

ceph-deploy osd create ceph-osd4 --data /dev/sdb
ceph-deploy osd create ceph-osd4 --data /dev/sdc
ceph-deploy osd create ceph-osd4 --data /dev/sdd

### éªŒè¯osdèŠ‚ç‚¹æœåŠ¡
root@ceph-osd1:~# ps -ef| grep osd
ceph       20905       1  0 11:34 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph
ceph       22649       1  0 11:34 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph
ceph       24414       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph
ceph       26150       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph
ceph       27883       1  0 11:35 ?        00:00:00 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph
root       28410    6825  0 11:35 pts/0    00:00:00 grep --color=auto osd

### éªŒè¯cephé›†ç¾¤çŠ¶æ€
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph -s
  cluster:
    id:     4e91b0f6-fc42-40e3-b9dd-53d0a0204fae
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-mon1,ceph-mon2,ceph-mon3 (age 14m)
    mgr: ceph-mgr1(active, since 9m), standbys: ceph-mgr2
    osd: 18 osds: 18 up (since 10s), 18 in (since 19s)

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   116 MiB used, 288 GiB / 288 GiB avail
    pgs:     1 active+clean
</code></pre>
<h2 id="æµ‹è¯•ä¸Šä¼ äºä¸‹è½½æ•°æ®">æµ‹è¯•ä¸Šä¼ äºä¸‹è½½æ•°æ®</h2>
<pre><code class="language-bash"># åˆ›å»ºpool,åˆ†æˆ32ä¸ªPGï¼ŒPGæ˜¯è·¨ä¸»æœºçš„
ceph osd pool create $pool_name 32 32
ceph osd pool ls
ceph pg ls-by-pool $pool_name | awk '{print $1,$2,$15}'
ceph osd tree
# æµ‹è¯•ä½¿ç”¨radosä¸Šä¼ æ–‡ä»¶åˆ°ceph
sudo rados put msg1 /var/log/syslog --pool=mypool
ceph osd map mypool msg1
</code></pre>
<h2 id="é›†ç¾¤æ‰©å®¹èŠ‚ç‚¹ç§»é™¤osdæ“ä½œ">é›†ç¾¤æ‰©å®¹èŠ‚ç‚¹/ç§»é™¤osdæ“ä½œ</h2>
<h3 id="æ‰©å±•ceph-monèŠ‚ç‚¹">æ‰©å±•ceph-monèŠ‚ç‚¹</h3>
<pre><code class="language-bash">mon2# apt install ceph-mon
mon3# apt install ceph-mon
deploy# ceph-deploy mon add  mon2 mon3
</code></pre>
<h3 id="æ‰©å±•ceph-mgrèŠ‚ç‚¹">æ‰©å±•ceph-mgrèŠ‚ç‚¹</h3>
<pre><code class="language-bash">mgr2# apt install ceph-mgr
deploy# ceph-deploy mgr create mgr2
</code></pre>
<h3 id="ä»æœåŠ¡å™¨ä¸­æ–°å¢ç§»é™¤osd">ä»æœåŠ¡å™¨ä¸­æ–°å¢/ç§»é™¤osd</h3>
<pre><code class="language-bash">### æ–°å¢OSD
æ–¹æ³•åŒä¸Šè¿°éƒ¨ç½²OSDè¿‡ç¨‹ã€‚

### ç§»é™¤osd
deploy # ceph osd out {osd-num}
osd# sudo systemctl stop ceph-osd@{osd-num}
osd# ceph osd purge {id} --yes-i-really-mean-it

</code></pre>
<h1 id="cephé›†ç¾¤åº”ç”¨åŸºç¡€">cephé›†ç¾¤åº”ç”¨åŸºç¡€</h1>
<h2 id="å—è®¾å¤‡-rbd">å—è®¾å¤‡ RBD</h2>
<p>RBDï¼ˆRADOS Block Device ï¼‰ä¸ºå—å­˜å‚¨çš„ä¸€ç§ï¼ŒRBDé€šè¿‡librbdåº“ä¸OSDäº¤äº’ã€‚</p>
<h3 id="åˆ›å»ºå—è®¾å¤‡å­˜å‚¨rbd">åˆ›å»ºå—è®¾å¤‡å­˜å‚¨RBD</h3>
<pre><code class="language-bash">### åˆ›å»ºå­˜å‚¨æ± 
# ceph osd pool create &lt;poolname&gt; pg_num pgp_num {replicated|erasure}

### åˆ›å»ºä¸€ä¸ªåä¸ºmyrbd1,pg/pgpä¸º64å¤§å°çš„å­˜å‚¨æ± 
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph osd pool create myrbdpool 64 64
pool 'myrbdpool' created
### å¯¹å­˜å‚¨æ± å¯ç”¨rbdåŠŸèƒ½
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph osd pool application enable myrbdpool rbd
enabled application 'rbd' on pool 'myrbdpool'

### å¯¹å­˜å‚¨æ± åˆå§‹åŒ–
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd pool init -p myrbdpool

### åœ¨RBDå­˜å‚¨æ± ä¸­åˆ›å»º2ä¸ªé•œåƒ(myimg2å¯ç”¨äºä½ç‰ˆæœ¬å†…æ ¸æŒ‚è½½æµ‹è¯•ï¼Œä»…å¼€å¯layeringç‰¹æ€§)
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd create myimg1 --size 2G --pool myrbdpool
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd create myimg2 --size 3G --pool myrbdpool --image-format 2 --image-feature layering

### æŸ¥çœ‹rbdå­˜å‚¨æ± é•œåƒä¿¡æ¯
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd ls --pool myrbdpool
myimg1
myimg2

ceph-admin@ceph-deploy:~/ceph-cluster$ rbd --image myimg1 --pool myrbdpool info
rbd image 'myimg1':
	size 2 GiB in 512 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 127673be7816
	block_name_prefix: rbd_data.127673be7816
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Sun Aug 22 23:18:30 2021
	access_timestamp: Sun Aug 22 23:18:30 2021
	modify_timestamp: Sun Aug 22 23:18:30 2021
ceph-admin@ceph-deploy:~/ceph-cluster$ rbd --image myimg2 --pool myrbdpool info
rbd image 'myimg2':
	size 3 GiB in 768 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 60aab8f8a2bb
	block_name_prefix: rbd_data.60aab8f8a2bb
	format: 2
	features: layering
	op_features:
	flags:
	create_timestamp: Sun Aug 22 23:19:14 2021
	access_timestamp: Sun Aug 22 23:19:14 2021
	modify_timestamp: Sun Aug 22 23:19:14 2021
</code></pre>
<h3 id="å®¢æˆ·ç«¯adminè´¦æˆ·æŒ‚è½½ä½¿ç”¨å—å­˜å‚¨">å®¢æˆ·ç«¯adminè´¦æˆ·æŒ‚è½½ä½¿ç”¨å—å­˜å‚¨</h3>
<pre><code class="language-bash">### å®¢æˆ·ç«¯éœ€è¦å®‰è£…ceph-commonï¼Œå¹¶åœ¨ /etc/ceph/ä¸‹å­˜åœ¨ ceph.client.admin.keyring è®¤è¯æ–‡ä»¶
ceph-admin@ceph-deploy:/etc/ceph$ ll /etc/ceph/
total 20
drwxr-xr-x    2 root root 4096 Aug 22 22:51 ./
drwxr-xr-x  100 root root 4096 Aug 22 22:22 ../
-rw-rw----+   1 root root  151 Aug 22 22:51 ceph.client.admin.keyring
-rw-r--r--    1 root root  307 Aug 22 22:51 ceph.conf
-rw-r--r--    1 root root   92 Jul  8 22:17 rbdmap
-rw-------    1 root root    0 Aug 22 22:51 tmpW61qMP

### å®¢æˆ·ç«¯æ˜ å°„img
#### æŒ‚è½½myimg1ï¼Œæç¤ºéƒ¨åˆ†ç‰¹æ€§æ— æ³•æ”¯æŒ(ä½¿ç”¨å†…æ ¸ç‰ˆæœ¬ä¸º4.15.0-112)
root@ceph-deploy:~# rbd -p myrbdpool map myimg1
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable myrbdpool/myimg1 object-map fast-diff deep-flatten&quot;.
In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.
rbd: map failed: (6) No such device or address

root@ceph-deploy:~# dmesg | tail
[ 9546.995674] rbd: image myimg1: image uses unsupported features: 0x38
#### æŒ‚è½½myimg2, ä»…å¼€å¯éƒ¨åˆ†ç‰¹æ€§ï¼Œå¯ä»¥æŒ‚è½½
root@ceph-deploy:~# rbd -p myrbdpool map myimg2
/dev/rbd0

### æ£€æŸ¥æŒ‚è½½æƒ…å†µ
root@ceph-deploy:~# lsblk
NAME                   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                      8:0    0  100G  0 disk
â””â”€sda1                   8:1    0  100G  0 part
  â”œâ”€ubu1804--vg-root   253:0    0   99G  0 lvm  /
  â””â”€ubu1804--vg-swap_1 253:1    0  980M  0 lvm  [SWAP]
sr0                     11:0    1  951M  0 rom
rbd0                   252:0    0    3G  0 disk

### æ ¼å¼åŒ–å¹¶å°è¯•å†™å…¥æ•°æ®
root@ceph-deploy:~# mkfs.ext4 /dev/rbd0
mke2fs 1.44.1 (24-Mar-2018)
Discarding device blocks: done
Creating filesystem with 786432 4k blocks and 196608 inodes
Filesystem UUID: 31f10239-3088-490a-9544-ebda0e5379b0
Superblock backups stored on blocks:
	32768, 98304, 163840, 229376, 294912

Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done

root@ceph-deploy:~# mkdir /cephrbd
root@ceph-deploy:~# mount /dev/rbd0 /cephrbd/
root@ceph-deploy:~# cp /var/log/syslog /cephrbd/
root@ceph-deploy:~# ll /cephrbd/
total 212
drwxr-xr-x  3 root root   4096 Aug 22 23:32 ./
drwxr-xr-x 24 root root   4096 Aug 22 23:31 ../
drwx------  2 root root  16384 Aug 22 23:31 lost+found/
-rw-r-----  1 root root 191373 Aug 22 23:32 syslog
root@ceph-deploy:/cephrbd# dd if=/dev/zero of=/cephrbd/ceph-testfile bs=1MB count=300
300+0 records in
300+0 records out
300000000 bytes (300 MB, 286 MiB) copied, 0.358303 s, 837 MB/s
root@ceph-deploy:/cephrbd# dd if=/dev/zero of=/cephrbd/ceph-testfile2 bs=64K count=3000
3000+0 records in
3000+0 records out
196608000 bytes (197 MB, 188 MiB) copied, 0.113061 s, 1.7 GB/s

root@ceph-deploy:/cephrbd# ls -alh
total 474M
drwxr-xr-x  3 root root 4.0K Aug 22 23:33 .
drwxr-xr-x 24 root root 4.0K Aug 22 23:31 ..
-rw-r--r--  1 root root 287M Aug 22 23:33 ceph-testfile
-rw-r--r--  1 root root 188M Aug 22 23:33 ceph-testfile2
drwx------  2 root root  16K Aug 22 23:31 lost+found
-rw-r-----  1 root root 187K Aug 22 23:32 syslog
</code></pre>
<h3 id="å®¢æˆ·ç«¯æ™®é€šè´¦æˆ·æŒ‚è½½ä½¿ç”¨å—å­˜å‚¨">å®¢æˆ·ç«¯æ™®é€šè´¦æˆ·æŒ‚è½½ä½¿ç”¨å—å­˜å‚¨</h3>
<ol>
<li>åˆ›å»ºæ™®é€šç”¨æˆ·ï¼Œå¹¶ç”Ÿæˆkeyringæ–‡ä»¶</li>
</ol>
<pre><code class="language-bash">### åˆ›å»ºæ™®é€šç”¨æˆ·user1, å¹¶å¯¹ myrbdpool å­˜å‚¨æ± æ·»åŠ æƒé™
ceph-admin@ceph-deploy:~$ ceph auth add client.user1 mon 'allow r' osd 'allow rwx pool=myrbdpool'
added key for client.user1
#### æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯
ceph-admin@ceph-deploy:~$ ceph auth get client.user1
[client.user1]
	key = AQBmtiNhCKWpFRAApg3x1RvMqHNjU+JCiOhjsQ==
	caps mon = &quot;allow r&quot;
	caps osd = &quot;allow rwx pool=myrbdpool&quot;
exported keyring for client.user1
#### å¯¼å‡ºç”¨æˆ·ç§˜é’¥æ–‡ä»¶
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph-authtool --create-keyring ceph.client.user1.keyring
creating ceph.client.user1.keyring
ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 300
drwxrwxr-x 2 ceph-admin ceph-admin   4096 Aug 23 22:54 ./
drwxr-xr-x 6 ceph-admin ceph-admin   4096 Aug 22 23:01 ../
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mds.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mgr.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-osd.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-rgw.keyring
-rw------- 1 ceph-admin ceph-admin    151 Aug 22 22:51 ceph.client.admin.keyring
-rw------- 1 ceph-admin ceph-admin      0 Aug 23 22:54 ceph.client.user1.keyring
-rw-rw-r-- 1 ceph-admin ceph-admin    307 Aug 22 22:37 ceph.conf
-rw-rw-r-- 1 ceph-admin ceph-admin 258705 Aug 22 23:05 ceph-deploy-ceph.log
-rw------- 1 ceph-admin ceph-admin     73 Aug 22 22:37 ceph.mon.keyring
-rwxrwxr-x 1 ceph-admin ceph-admin    897 Aug 22 23:01 ceph_osdcreate.sh*
-rwxrwxr-x 1 ceph-admin ceph-admin    735 Aug 22 23:00 ceph_osdzap.sh*
ceph-admin@ceph-deploy:~/ceph-cluster$ ceph auth get client.user1 -o ceph.client.user1.keyring
exported keyring for client.user1
ceph-admin@ceph-deploy:~/ceph-cluster$ ll
total 304
drwxrwxr-x 2 ceph-admin ceph-admin   4096 Aug 23 22:54 ./
drwxr-xr-x 6 ceph-admin ceph-admin   4096 Aug 22 23:01 ../
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mds.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-mgr.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-osd.keyring
-rw------- 1 ceph-admin ceph-admin    113 Aug 22 22:51 ceph.bootstrap-rgw.keyring
-rw------- 1 ceph-admin ceph-admin    151 Aug 22 22:51 ceph.client.admin.keyring
-rw------- 1 ceph-admin ceph-admin    124 Aug 23 22:55 ceph.client.user1.keyring
-rw-rw-r-- 1 ceph-admin ceph-admin    307 Aug 22 22:37 ceph.conf
-rw-rw-r-- 1 ceph-admin ceph-admin 258705 Aug 22 23:05 ceph-deploy-ceph.log
-rw------- 1 ceph-admin ceph-admin     73 Aug 22 22:37 ceph.mon.keyring
-rwxrwxr-x 1 ceph-admin ceph-admin    897 Aug 22 23:01 ceph_osdcreate.sh*
-rwxrwxr-x 1 ceph-admin ceph-admin    735 Aug 22 23:00 ceph_osdzap.sh*
ceph-admin@ceph-deploy:~/ceph-cluster$ cat ceph.client.user1.keyring
[client.user1]
	key = AQBmtiNhCKWpFRAApg3x1RvMqHNjU+JCiOhjsQ==
	caps mon = &quot;allow r&quot;
	caps osd = &quot;allow rwx pool=myrbdpool&quot;

</code></pre>
<ol>
<li>åœ¨å®¢æˆ·ç«¯ä¸­å®‰è£…ceph-commonå®¢æˆ·ç«¯ï¼ˆå¯¹åº”Cephç‰ˆæœ¬ï¼‰</li>
<li>å°†ç”¨æˆ·keyringæ–‡ä»¶åŒæ­¥åˆ°/etc/ceph/ç›®å½•ä¸‹</li>
</ol>
<pre><code class="language-bash">ceph-admin@ceph-deploy:~/ceph-cluster$ rsync -avzPr ceph.client.user1.keyring root@ceph-osd4:/etc/ceph/
root@ceph-osd4's password:
sending incremental file list
ceph.client.user1.keyring
            124 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/1)

sent 239 bytes  received 35 bytes  49.82 bytes/sec
total size is 124  speedup is 0.45
</code></pre>
<ol>
<li>åœ¨å®¢æˆ·ç«¯éªŒè¯æƒé™ï¼Œå¹¶æŒ‚è½½é•œåƒ</li>
</ol>
<pre><code class="language-bash">root@ceph-osd4:~# ll /etc/ceph/
total 24
drwxr-xr-x   2 root       root       4096 Aug 23 22:59 ./
drwxr-xr-x  99 root       root       4096 Aug 22 22:59 ../
-rw-rw----+  1 root       root        151 Aug 22 22:52 ceph.client.admin.keyring
-rw-------   1 ceph-admin ceph-admin  124 Aug 23 22:55 ceph.client.user1.keyring
-rw-r--r--   1 root       root        307 Aug 22 23:05 ceph.conf
-rw-r--r--   1 root       root         92 Jul  8 22:17 rbdmap
-rw-------   1 root       root          0 Aug 22 22:52 tmp5Reg6c

### æ˜ å°„rbdä¸º /dev/rbd{x}
root@ceph-osd4:/# rbd --user user1 -p myrbdpool map myimg2
/dev/rbd0
rbd: --user is deprecated, use --id
### å¦‚ä¸ºæ–°åˆ›å»ºé•œåƒï¼Œåˆ™å…ˆæ ¼å¼åŒ–é•œåƒ
root@ceph-osd4:/# mkfs.ext4 /dev/rbd0
### æŒ‚è½½é•œåƒ
root@ceph-osd4:/# mount /dev/rbd0 /ceph-rbd/
root@ceph-osd4:/# cd /ceph-rbd/
root@ceph-osd4:/ceph-rbd# ll
total 485184
drwxr-xr-x  3 root root      4096 Aug 22 23:33 ./
drwxr-xr-x 24 root root      4096 Aug 23 23:00 ../
-rw-r--r--  1 root root 300000000 Aug 22 23:33 ceph-testfile
-rw-r--r--  1 root root 196608000 Aug 22 23:33 ceph-testfile2
drwx------  2 root root     16384 Aug 22 23:31 lost+found/
-rw-r-----  1 root root    191373 Aug 22 23:32 syslog
</code></pre>
<h3 id="æ‰©å®¹rbdé•œåƒ">æ‰©å®¹rbdé•œåƒ</h3>
<pre><code class="language-bash">### æŸ¥çœ‹å½“å‰é•œåƒå¤§å°
root@ceph-deploy:/# rbd ls -p myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  3 GiB            2
### ä»3Gæ‰©å®¹åˆ°5G
root@ceph-deploy:/# rbd resize --pool myrbdpool --image myimg2 --size 5G
Resizing image: 100% complete...done.
root@ceph-deploy:/# rbd ls -p myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  5 GiB            2
### éªŒè¯å®¢æˆ·ç«¯æ‰©å®¹ç»“æœ
root@ceph-osd4:/ceph-rbd# fdisk -l /dev/rbd0
Disk /dev/rbd0: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes
root@ceph-osd4:/ceph-rbd# df -Th
Filesystem                   Type      Size  Used Avail Use% Mounted on
udev                         devtmpfs  966M     0  966M   0% /dev
tmpfs                        tmpfs     200M  8.9M  191M   5% /run
/dev/mapper/ubu1804--vg-root ext4       97G  2.7G   90G   3% /
tmpfs                        tmpfs     997M     0  997M   0% /dev/shm
tmpfs                        tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs                        tmpfs     997M     0  997M   0% /sys/fs/cgroup
tmpfs                        tmpfs     200M     0  200M   0% /run/user/1000
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-15
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-16
tmpfs                        tmpfs     997M   68K  997M   1% /var/lib/ceph/osd/ceph-17
/dev/rbd0                    ext4      2.9G  483M  2.3G  18% /ceph-rbd

</code></pre>
<h3 id="å¸è½½rbdé•œåƒ">å¸è½½RBDé•œåƒ</h3>
<pre><code class="language-bash"># umount /ceph-rbd
# rbd --id user1 -p myrbdpool unmap myimg2

</code></pre>
<h3 id="åˆ é™¤rbdé•œåƒ">åˆ é™¤RBDé•œåƒ</h3>
<pre><code class="language-bash"># rbd rm --pool myrbdpool --image myimg2
</code></pre>
<h3 id="rbdé•œåƒå›æ”¶ç«™">RBDé•œåƒå›æ”¶ç«™</h3>
<p>å¯ä»¥å°†éœ€è¦åˆ é™¤çš„é•œåƒå…ˆç§»åŠ¨åˆ°å›æ”¶ç«™ï¼ŒåæœŸç¡®è®¤åˆ é™¤æ—¶å†è¿›è¡Œæ°¸ä¹…åˆ é™¤ã€‚</p>
<pre><code class="language-bash">### æŸ¥çœ‹é•œåƒçŠ¶æ€
root@ceph-deploy:/# rbd status --pool myrbdpool --image myimg1
Watchers: none
root@ceph-deploy:/# rbd status --pool myrbdpool --image myimg2
Watchers:
	watcher=10.0.11.109:0/3891133225 client.4938 cookie=18446462598732840961
	watcher=10.0.11.100:0/1430785202 client.4789 cookie=18446462598732840962
root@ceph-deploy:/# rbd trash move --pool myrbdpool --image myimg2
root@ceph-deploy:/# rbd trash list --pool myrbdpool
60aab8f8a2bb myimg2
root@ceph-deploy:/# rbd trash restore --pool myrbdpool --image myimg2 --image-id 60aab8f8a2bb
root@ceph-deploy:/# rbd ls --pool myrbdpool -l
NAME    SIZE   PARENT  FMT  PROT  LOCK
myimg1  2 GiB            2
myimg2  5 GiB            2
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://vincentgresham.github.io/post/hello-gridea/</id>
        <link href="https://vincentgresham.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>